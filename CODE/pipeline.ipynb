{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Pipeline ShapeWorks - Version Optimisée\n",
    "\n",
    "\n",
    "\n",
    " Dans cette version, **le grooming est effectué une seule fois** pour l'ensemble des données.\n",
    "\n",
    " Ensuite, pour chaque jeu de paramètres variables (dans la grille de `GRID_PARAMS`), **nous ré-exécutons uniquement les étapes d'optimisation, d'analyse PCA et de calcul de métriques**, en réutilisant les résultats du grooming déjà sauvegardés sur disque.\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    "\n",
    "\n",
    " ## Changements Demandés\n",
    "\n",
    " 1. **Réorganiser le code** pour éviter la redondance entre pipeline \"avec grooming\" et \"sans grooming\".\n",
    "\n",
    "    - On conserve des fonctions de *pré-traitement* (acquisition, grooming, rigid...) et des fonctions d'*optimisation / analyse*.\n",
    "\n",
    "    - On n'a plus deux grosses fonctions pipeline distinctes, mais plutôt un flux clair :\n",
    "\n",
    "      1) `run_preprocessing()` (acquire_data + groom_shapes + rigid_transformations)\n",
    "\n",
    "      2) Pour chaque paramètre : `run_optimization_and_analysis()` (optimize + PCA + metrics)\n",
    "\n",
    "\n",
    "\n",
    " 2. **Format des temps** dans l'Excel : au lieu de minutes entières, on veut `minutes:secondes`, pour voir également les petites durées.\n",
    "\n",
    " 3. **Ajouter 3 colonnes** supplémentaires dans la feuille \"PARAMS\" (le tableau principal) :\n",
    "\n",
    "    - `compactness_95` : nombre de composantes nécessaires pour atteindre 95% de la variance\n",
    "\n",
    "    - `final_specificity_error` : la dernière valeur de l'erreur de spécificité\n",
    "\n",
    "    - `final_generalization_error` : la dernière valeur de l'erreur de généralisation\n",
    "\n",
    "\n",
    "\n",
    " Le reste du code doit rester **identique** dans son fonctionnement et ses étapes, car la pipeline initiale « fonctionne parfaitement »."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Imports et Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import shapeworks as sw\n",
    "\n",
    "# Pour tracer et insérer des images dans Excel\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "import tempfile\n",
    "from openpyxl.styles import Alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 0. Paramètres Généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Dossiers ----------\n",
    "DATASET_NAME       = \"TEST\"\n",
    "DATASET_PATHS      = [\n",
    "    # (os.path.join(\".\", \"DATA\", \"RF_FULGUR_M\"), \"RF\"),\n",
    "    (os.path.join(\".\", \"DATA\", \"RF_FULGUR_SAMPLE_2\"), \"RF_S2\") \n",
    "]\n",
    "SHAPE_EXT          = \".nii.gz\"\n",
    "\n",
    "# Sorties dans \"./OUTPUT_PIPELINE\"\n",
    "BASE_OUTPUT_DIR    = os.path.abspath(os.path.join(\".\", \"OUTPUT_PIPELINE\"))\n",
    "OPTION_SET         = \"default\"  # Pour nommer les fichiers\n",
    "\n",
    "# ---------- Paramètres Grooming ----------\n",
    "ANTIALIAS_ITERATIONS = 30\n",
    "ISO_SPACING          = [1, 1, 1]\n",
    "PAD_SIZE             = 10\n",
    "PAD_VALUE            = 0\n",
    "ISO_VALUE            = 0.5\n",
    "ICP_ITERATIONS       = 100\n",
    "\n",
    "# ---------- Paramètres par Défaut ----------\n",
    "OPT_PARAMS = {\n",
    "    \"number_of_particles\":       128,\n",
    "    \"use_normals\":               0,\n",
    "    \"normals_strength\":          10.0,\n",
    "    \"checkpointing_interval\":    1000,\n",
    "    \"keep_checkpoints\":          0,\n",
    "    \"iterations_per_split\":      1000,\n",
    "    \"optimization_iterations\":   1000,\n",
    "    \"starting_regularization\":   10,\n",
    "    \"ending_regularization\":     1,\n",
    "    \"relative_weighting\":        1,\n",
    "    \"initial_relative_weighting\":0.05,\n",
    "    \"procrustes_interval\":       0,\n",
    "    \"procrustes_scaling\":        0,\n",
    "    \"save_init_splits\":          0,\n",
    "    \"verbosity\":                 0,\n",
    "    \"multiscale\":                1,\n",
    "    \"multiscale_particles\":      32,\n",
    "    \"tiny_test\":                 False,\n",
    "    \"use_single_scale\":          0,\n",
    "    \"mesh_mode\":                 True,  # True => optimisation sur des meshes\n",
    "    \"option_set\":                OPTION_SET\n",
    "}\n",
    "\n",
    "# ---------- Paramètres Variables (Grid) ----------\n",
    "GRID_PARAMS = {\n",
    "    \"number_of_particles\": [16, 32, 64, 128]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Préparation du dossier de sortie ----------\n",
    "OUTPUT_DB = os.path.abspath(os.path.join(\".\", \"OUTPUT_DB\"))\n",
    "os.makedirs(OUTPUT_DB, exist_ok=True)\n",
    "\n",
    "if os.path.exists(BASE_OUTPUT_DIR):\n",
    "    # Déplacer les fichiers Excel pré-existants\n",
    "    for root, dirs, files in os.walk(BASE_OUTPUT_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                source_file = os.path.join(root, file)\n",
    "                new_filename = f\"{os.path.splitext(file)[0]}_{int(time.time())}.xlsx\"\n",
    "                dest_file = os.path.join(OUTPUT_DB, new_filename)\n",
    "                shutil.move(source_file, dest_file)\n",
    "    # Supprimer le dossier complet pour repartir propre\n",
    "    shutil.rmtree(BASE_OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Fonctions : Acquisition, Grooming, Rigid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_data(dataset_paths, shape_ext, output_path):\n",
    "    \"\"\"\n",
    "    Récupère et trie les noms de fichiers .nii.gz\n",
    "    Renvoie (shape_filenames, dataset_ids)\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 1. Acquire Data\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    shape_filenames = []\n",
    "    dataset_ids = []\n",
    "\n",
    "    for data_path, dataset_id in dataset_paths:\n",
    "        files = sorted(glob.glob(os.path.join(data_path, '*' + shape_ext)))\n",
    "        shape_filenames.extend(files)\n",
    "        dataset_ids.extend([dataset_id] * len(files))\n",
    "\n",
    "    print(f\"Nombre de shapes : {len(shape_filenames)}\")\n",
    "    return shape_filenames, dataset_ids\n",
    "\n",
    "\n",
    "def groom_shapes(shape_filenames, dataset_ids, groom_dir):\n",
    "    \"\"\"\n",
    "    1) Crop bounding box\n",
    "    2) Antialias + resample + binarize\n",
    "    3) Pad\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 2. Groom - Data Pre-processing\")\n",
    "\n",
    "    os.makedirs(groom_dir, exist_ok=True)\n",
    "\n",
    "    shape_seg_list = []\n",
    "    shape_names = []\n",
    "\n",
    "    for i, shape_filename in enumerate(tqdm(shape_filenames, desc=\"Grooming shapes\")):\n",
    "        dataset_id = dataset_ids[i]\n",
    "        base_shape_name = os.path.basename(shape_filename).replace(SHAPE_EXT, '')\n",
    "        shape_name = f\"{dataset_id}_{base_shape_name}\"\n",
    "        shape_names.append(shape_name)\n",
    "\n",
    "        shape_seg = sw.Image(shape_filename)\n",
    "        shape_seg_list.append(shape_seg)\n",
    "\n",
    "        bounding_box = sw.ImageUtils.boundingBox([shape_seg], ISO_VALUE).pad(2)\n",
    "        shape_seg.crop(bounding_box)\n",
    "\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).resample(ISO_SPACING, sw.InterpolationType.Linear).binarize()\n",
    "        shape_seg.pad(PAD_SIZE, PAD_VALUE)\n",
    "\n",
    "    return shape_seg_list, shape_names\n",
    "\n",
    "\n",
    "def rigid_transformations(shape_seg_list, shape_names, groom_dir):\n",
    "    \"\"\"\n",
    "    1) Trouve l'image de référence\n",
    "    2) Calcule la rigid transform\n",
    "    3) Convertit en distance transform (antialias->DT->blur)\n",
    "    4) Sauvegarde en .nrrd\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 3. Groom - Rigid Transformations\")\n",
    "\n",
    "    ref_index = sw.find_reference_image_index(shape_seg_list)\n",
    "    ref_seg = shape_seg_list[ref_index]\n",
    "    ref_name = shape_names[ref_index]\n",
    "\n",
    "    os.makedirs(groom_dir, exist_ok=True)\n",
    "\n",
    "    ref_filename = os.path.join(groom_dir, 'reference.nrrd')\n",
    "    ref_seg.write(ref_filename)\n",
    "    print(f\"Image de référence trouvée : {ref_name}\")\n",
    "\n",
    "    transform_dir = os.path.join(groom_dir, 'rigid_transforms')\n",
    "    os.makedirs(transform_dir, exist_ok=True)\n",
    "\n",
    "    rigid_transforms = []\n",
    "\n",
    "    for shape_seg, shape_name in tqdm(zip(shape_seg_list, shape_names),\n",
    "                                      desc=\"Calcul des transformations\",\n",
    "                                      total=len(shape_seg_list)):\n",
    "        rigid_transform = shape_seg.createRigidRegistrationTransform(ref_seg, ISO_VALUE, ICP_ITERATIONS)\n",
    "        rigid_transform = sw.utils.getVTKtransform(rigid_transform)\n",
    "        rigid_transforms.append(rigid_transform)\n",
    "\n",
    "        transform_filename = os.path.join(transform_dir, f'{shape_name}_to_{ref_name}_transform.txt')\n",
    "        np.savetxt(transform_filename, rigid_transform)\n",
    "\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).computeDT(0).gaussianBlur(1.5)\n",
    "\n",
    "    # Sauvegarde finale en .nrrd\n",
    "    output_subdir = 'distance_transforms'\n",
    "    output_extension = '.nrrd'\n",
    "    output_dir = os.path.join(groom_dir, output_subdir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    groomed_files = []\n",
    "    for shape_seg, shape_name in zip(shape_seg_list, shape_names):\n",
    "        out_name = os.path.join(output_dir, shape_name + output_extension)\n",
    "        shape_seg.write(out_name)\n",
    "        groomed_files.append(out_name)\n",
    "\n",
    "    return rigid_transforms, groomed_files\n",
    "\n",
    "\n",
    "def run_preprocessing(dataset_paths, shape_ext, base_dir):\n",
    "    \"\"\"\n",
    "    Orchestrateur pour la partie 'grooming':\n",
    "      - acquisition\n",
    "      - grooming\n",
    "      - rigid\n",
    "    Retourne : shape_seg_list, shape_filenames, dataset_ids, shape_names, rigid_transforms, groomed_files\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(base_dir, \"OUTPUT\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    groom_dir = os.path.join(output_path, \"groomed\")\n",
    "\n",
    "    shape_filenames, dataset_ids = acquire_data(dataset_paths, shape_ext, output_path)\n",
    "    shape_seg_list, shape_names = groom_shapes(shape_filenames, dataset_ids, groom_dir)\n",
    "    rigid_transforms, groomed_files = rigid_transformations(shape_seg_list, shape_names, groom_dir)\n",
    "\n",
    "    return shape_seg_list, shape_filenames, dataset_ids, shape_names, rigid_transforms, groomed_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Fonctions : Optimisation, PCA, Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_particles(shape_seg_list, shape_filenames, rigid_transforms, groomed_files, output_path):\n",
    "    \"\"\"\n",
    "    Step 4 : Optimisation\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 4. Optimize - Particle Based Optimization\")\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # -- Récupération domain_type + conversion si mesh_mode --\n",
    "    domain_type, groomed_files_out = sw.data.get_optimize_input(\n",
    "        groomed_files,\n",
    "        OPT_PARAMS[\"mesh_mode\"]\n",
    "    )\n",
    "\n",
    "    # Construction sujets\n",
    "    subjects = []\n",
    "    for i in range(len(shape_seg_list)):\n",
    "        subj = sw.Subject()\n",
    "        subj.set_number_of_domains(1)\n",
    "\n",
    "        subj.set_original_filenames([os.path.abspath(shape_filenames[i])])\n",
    "        subj.set_groomed_filenames([os.path.abspath(groomed_files_out[i])])\n",
    "        subj.set_groomed_transforms([rigid_transforms[i].flatten()])\n",
    "\n",
    "        try:\n",
    "            subj.set_domain_type(0, domain_type)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        subjects.append(subj)\n",
    "\n",
    "    # Projet\n",
    "    project = sw.Project()\n",
    "    project.set_subjects(subjects)\n",
    "    parameters = sw.Parameters()\n",
    "\n",
    "    # Paramètres\n",
    "    valid_params = {\n",
    "        \"number_of_particles\":        OPT_PARAMS[\"number_of_particles\"],\n",
    "        \"use_normals\":                OPT_PARAMS[\"use_normals\"],\n",
    "        \"normals_strength\":           OPT_PARAMS[\"normals_strength\"],\n",
    "        \"checkpointing_interval\":     OPT_PARAMS[\"checkpointing_interval\"],\n",
    "        \"keep_checkpoints\":           OPT_PARAMS[\"keep_checkpoints\"],\n",
    "        \"iterations_per_split\":       OPT_PARAMS[\"iterations_per_split\"],\n",
    "        \"optimization_iterations\":    OPT_PARAMS[\"optimization_iterations\"],\n",
    "        \"starting_regularization\":    OPT_PARAMS[\"starting_regularization\"],\n",
    "        \"ending_regularization\":      OPT_PARAMS[\"ending_regularization\"],\n",
    "        \"relative_weighting\":         OPT_PARAMS[\"relative_weighting\"],\n",
    "        \"initial_relative_weighting\": OPT_PARAMS[\"initial_relative_weighting\"],\n",
    "        \"procrustes_interval\":        OPT_PARAMS[\"procrustes_interval\"],\n",
    "        \"procrustes_scaling\":         OPT_PARAMS[\"procrustes_scaling\"],\n",
    "        \"save_init_splits\":           OPT_PARAMS[\"save_init_splits\"],\n",
    "        \"verbosity\":                  OPT_PARAMS[\"verbosity\"]\n",
    "    }\n",
    "\n",
    "    # tiny_test\n",
    "    if OPT_PARAMS.get(\"tiny_test\", False):\n",
    "        valid_params[\"number_of_particles\"] = 32\n",
    "        valid_params[\"optimization_iterations\"] = 25\n",
    "\n",
    "    # multiscale\n",
    "    if not OPT_PARAMS.get(\"use_single_scale\", 0):\n",
    "        valid_params[\"multiscale\"] = 1\n",
    "        valid_params[\"multiscale_particles\"] = OPT_PARAMS[\"multiscale_particles\"]\n",
    "\n",
    "    # Remplissage\n",
    "    for k, v in valid_params.items():\n",
    "        parameters.set(k, sw.Variant([v]))\n",
    "\n",
    "    project.set_parameters(\"optimize\", parameters)\n",
    "\n",
    "    # Sauvegarde .swproj\n",
    "    proj_file = os.path.join(output_path, f\"{DATASET_NAME}_{OPTION_SET}.swproj\")\n",
    "    project.save(proj_file)\n",
    "\n",
    "    # Lancement de l'optim\n",
    "    print(\"Lancement de l'optimisation via shapeworks...\")\n",
    "    cmd = ['shapeworks', 'optimize', '--progress', '--name', proj_file]\n",
    "    subprocess.check_call(cmd, cwd=output_path)\n",
    "\n",
    "    # Verification\n",
    "    args_for_check = type('ArgsForCheck', (object,), {})()\n",
    "    args_for_check.tiny_test = OPT_PARAMS.get(\"tiny_test\", False)\n",
    "    args_for_check.verify    = False\n",
    "    sw.utils.check_results(args_for_check, proj_file)\n",
    "\n",
    "    return os.path.join(output_path, f\"{DATASET_NAME}_default_particles\")\n",
    "\n",
    "\n",
    "def get_particles(particles_dir, particle_type=\"world\"):\n",
    "    \"\"\"\n",
    "    Lit tous les fichiers *.world.particles\n",
    "    Retourne un tableau [n_shapes, n_points, 3] et la liste des noms.\n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    names = []\n",
    "    for filename in os.listdir(particles_dir):\n",
    "        if filename.endswith(particle_type + \".particles\"):\n",
    "            data = np.loadtxt(os.path.join(particles_dir, filename))\n",
    "            particles.append(data)\n",
    "            names.append(os.path.splitext(filename)[0])\n",
    "    if not particles:\n",
    "        return None, None\n",
    "    return np.array(particles), names\n",
    "\n",
    "\n",
    "def compute_pca(particles_dir, pca_output_dir):\n",
    "    \"\"\"\n",
    "    Step 5 : PCA\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Calcul de l'ACP\")\n",
    "    os.makedirs(pca_output_dir, exist_ok=True)\n",
    "\n",
    "    parts, names = get_particles(particles_dir, \"world\")\n",
    "    if parts is None:\n",
    "        raise ValueError(\"Aucune particule chargée depuis \" + particles_dir)\n",
    "\n",
    "    n, p, _ = parts.shape\n",
    "    parts_flat = parts.reshape(n, -1)\n",
    "    print(f\"Forme des particules aplaties : {parts_flat.shape}\")\n",
    "\n",
    "    pca = PCA(n_components=n - 1)\n",
    "    pca.fit(parts_flat)\n",
    "    comps = pca.transform(parts_flat)\n",
    "\n",
    "    # Eigenvalues\n",
    "    eigvals = pca.explained_variance_\n",
    "    with open(os.path.join(pca_output_dir, 'eigenvalues.eval'), 'w') as f:\n",
    "        for ev in eigvals:\n",
    "            f.write(f\"{ev}\\n\")\n",
    "\n",
    "    # Eigenvectors\n",
    "    eigenvectors = pca.components_\n",
    "    eigenvectors_reshaped = eigenvectors.reshape(eigenvectors.shape[0], -1, 3)\n",
    "    for i, eigvec in enumerate(eigenvectors_reshaped):\n",
    "        fn = os.path.join(pca_output_dir, f\"eigenvector_{i+1}.eig\")\n",
    "        np.savetxt(fn, eigvec, fmt='%f')\n",
    "\n",
    "    # Projection sur 2 composantes\n",
    "    pca_projection = comps[:, :2]\n",
    "\n",
    "    print(\"Résultats de la PCA sauvegardés dans\", pca_output_dir)\n",
    "    return pca_projection, eigvals\n",
    "\n",
    "\n",
    "def load_shapes(particles_dir, particle_type=\"world\"):\n",
    "    shapes = []\n",
    "    for filename in os.listdir(particles_dir):\n",
    "        if filename.endswith(particle_type + '.particles'):\n",
    "            shape = np.loadtxt(os.path.join(particles_dir, filename))\n",
    "            shapes.append(shape)\n",
    "    return np.array(shapes)\n",
    "\n",
    "\n",
    "def compute_compactness(eigenvalues, threshold=0.95):\n",
    "    total_var = np.sum(eigenvalues)\n",
    "    cum_var = np.cumsum(eigenvalues) / total_var\n",
    "    num_comp = int(np.argmax(cum_var >= threshold) + 1)\n",
    "    return num_comp, cum_var\n",
    "\n",
    "\n",
    "def compute_specificity(real_shapes, num_particles, num_samples=1000):\n",
    "    n, p, dim3 = real_shapes.shape\n",
    "    d = p * dim3\n",
    "    real_shapes_2d = real_shapes.reshape(n, d)\n",
    "\n",
    "    Y = real_shapes_2d.T\n",
    "    mu = np.mean(Y, axis=1, keepdims=True)\n",
    "    Yc = Y - mu\n",
    "    U, S, _ = np.linalg.svd(Yc, full_matrices=False)\n",
    "    if S[0] < S[-1]:\n",
    "        S = S[::-1]\n",
    "        U = np.fliplr(U)\n",
    "\n",
    "    specifics = np.zeros(n - 1)\n",
    "\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "\n",
    "    for m in tqdm(range(1, n), desc=\"Calcul de la spécificité\"):\n",
    "        epsi = U[:, :m]\n",
    "        stdevs = np.sqrt(S[:m])\n",
    "        betas = np.random.randn(m, num_samples)\n",
    "        for i_mode in range(m):\n",
    "            betas[i_mode, :] *= stdevs[i_mode]\n",
    "        synth = epsi @ betas + mu\n",
    "        min_dists = np.zeros(num_samples)\n",
    "        for isyn in range(num_samples):\n",
    "            sy = synth[:, isyn]\n",
    "            best = 1e15\n",
    "            for j in range(n):\n",
    "                dist_j = shape_distance(sy, Y[:, j], num_particles)\n",
    "                if dist_j < best:\n",
    "                    best = dist_j\n",
    "            min_dists[isyn] = best\n",
    "        specifics[m-1] = np.mean(min_dists) / float(num_particles)\n",
    "\n",
    "    return specifics\n",
    "\n",
    "\n",
    "def compute_generalization(real_shapes, num_particles):\n",
    "    if len(real_shapes.shape) == 3 and real_shapes.shape[2] == 3:\n",
    "        n, p, dim3 = real_shapes.shape\n",
    "        d = p * dim3\n",
    "        real_shapes_2d = real_shapes.reshape(n, d)\n",
    "    else:\n",
    "        n, d = real_shapes.shape\n",
    "        real_shapes_2d = real_shapes\n",
    "\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "\n",
    "    P = real_shapes_2d.T\n",
    "    gens = np.zeros(n - 1)\n",
    "\n",
    "    for m in range(1, n):\n",
    "        tot_dist = 0.0\n",
    "        for leave in range(n):\n",
    "            Y = np.zeros((P.shape[0], n-1))\n",
    "            Y[:, :leave] = P[:, :leave]\n",
    "            Y[:, leave:] = P[:, leave+1:]\n",
    "            mu = np.mean(Y, axis=1, keepdims=True)\n",
    "            Yc = Y - mu\n",
    "            U, S, _ = np.linalg.svd(Yc, full_matrices=False)\n",
    "            epsi = U[:, :m]\n",
    "\n",
    "            ytest = P[:, leave:leave+1]\n",
    "            betas = epsi.T @ (ytest - mu)\n",
    "            rec = epsi @ betas + mu\n",
    "\n",
    "            dist = shape_distance(rec, ytest, num_particles) / float(num_particles)\n",
    "            tot_dist += dist\n",
    "        gens[m - 1] = tot_dist / float(n)\n",
    "\n",
    "    return gens\n",
    "\n",
    "\n",
    "def compute_error_metrics(particles_dir, pca_output_dir, num_particles):\n",
    "    \"\"\"\n",
    "    Steps 5 & 6 : on calcule 3 métriques (compactness, specificity, generalization).\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Calcul des métriques d'erreur\")\n",
    "\n",
    "    real_shapes = load_shapes(particles_dir, \"world\")\n",
    "    if real_shapes.size == 0:\n",
    "        raise ValueError(f\"Aucune shape chargée dans {particles_dir}\")\n",
    "\n",
    "    eigenvalues_path = os.path.join(pca_output_dir, 'eigenvalues.eval')\n",
    "    if not os.path.exists(eigenvalues_path):\n",
    "        raise FileNotFoundError(\"Fichier eigenvalues.eval introuvable : \" + eigenvalues_path)\n",
    "    eigenvalues = np.loadtxt(eigenvalues_path)\n",
    "\n",
    "    # 1) Compactness\n",
    "    c_required, c_variance = compute_compactness(eigenvalues)\n",
    "\n",
    "    # 2) Specificity\n",
    "    specifics = compute_specificity(real_shapes, num_particles)\n",
    "\n",
    "    # 3) Generalization\n",
    "    generals = compute_generalization(real_shapes, num_particles)\n",
    "\n",
    "    metrics = {\n",
    "        \"compactness_required\": c_required,\n",
    "        \"cumulative_variance\": c_variance.tolist(),\n",
    "        \"specificity\": specifics.tolist(),\n",
    "        \"generalization\": generals.tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3. Fonction Principale : Optim + PCA + Métriques pour un set de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization_and_analysis(run_params, run_index,\n",
    "                                  shape_seg_list, shape_filenames,\n",
    "                                  rigid_transforms, groomed_files,\n",
    "                                  base_output_dir):\n",
    "    \"\"\"\n",
    "    Etapes :\n",
    "      - Optimisation\n",
    "      - PCA\n",
    "      - Metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"Lancement Optim+Analyse - Exécution {run_index}\")\n",
    "\n",
    "    overall_start = time.time()\n",
    "\n",
    "    # Sous-dossier pour ce run\n",
    "    run_dir = os.path.join(base_output_dir, f\"Run_{run_index}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    output_path = os.path.join(run_dir, \"OUTPUT\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    pca_out = os.path.join(output_path, \"PCA_results\")\n",
    "\n",
    "    # MàJ des OPT_PARAMS\n",
    "    OPT_PARAMS.update(run_params)\n",
    "\n",
    "    step_times = {}\n",
    "\n",
    "    # -- Step 4 : optimize --\n",
    "    t0 = time.time()\n",
    "    particles_dir = optimize_particles(\n",
    "        shape_seg_list,\n",
    "        shape_filenames,\n",
    "        rigid_transforms,\n",
    "        groomed_files,\n",
    "        output_path\n",
    "    )\n",
    "    step_times[\"optimization\"] = time.time() - t0\n",
    "\n",
    "    # -- Step 5 : PCA --\n",
    "    t0 = time.time()\n",
    "    pca_projection, eigenvalues = compute_pca(particles_dir, pca_out)\n",
    "    step_times[\"pca\"] = time.time() - t0\n",
    "\n",
    "    # -- Step 6 : metrics --\n",
    "    t0 = time.time()\n",
    "    n_parts = OPT_PARAMS.get(\"number_of_particles\", 128)\n",
    "    metrics = compute_error_metrics(particles_dir, pca_out, n_parts)\n",
    "    step_times[\"error_metrics\"] = time.time() - t0\n",
    "\n",
    "    overall_time = time.time() - overall_start\n",
    "    print(f\"Exécution {run_index} terminée en {overall_time:.2f} secondes\")\n",
    "\n",
    "    return {\n",
    "        \"pca_projection\": pca_projection,\n",
    "        \"metrics\": metrics,\n",
    "        \"params\": run_params,\n",
    "        \"step_times\": step_times,\n",
    "        \"total_execution_time\": overall_time\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4. Sauvegarde du fichier Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_ss_format(seconds):\n",
    "    mm = int(seconds // 60)\n",
    "    ss = int(seconds % 60)\n",
    "    return f\"{mm}:{ss:02d}\"\n",
    "\n",
    "\n",
    "def _plot_metric_curve(data, title, ylabel, run_idx, outdir, figsize=(5, 3)):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    fn = f\"{title.replace(' ', '_')}_Run_{run_idx}.png\"\n",
    "    image_path = os.path.join(outdir, fn)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(range(1, len(data) + 1), data, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Number of Modes\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.savefig(image_path, dpi=130, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image_path\n",
    "\n",
    "\n",
    "def save_results_to_excel(results, excel_filename, grid_keys):\n",
    "    \"\"\"\n",
    "    Génère le fichier Excel final.\n",
    "    - Feuille \"PARAMS\" : \n",
    "        * run index\n",
    "        * grid_keys\n",
    "        * time_grooming, time_rigid, time_optimization, time_total (au format mm:ss)\n",
    "        * + 3 colonnes : compactness_95, final_specificity_error, final_generalization_error\n",
    "        * plus, en bas, la liste multi-ligne des paramètres\n",
    "    - Feuilles \"Run_i\" :\n",
    "        * Graphes (Compactness, Specificity, Generalization)\n",
    "        * Valeurs PC1/PC2\n",
    "    \"\"\"\n",
    "    print(\"\\nSauvegarde des résultats dans le fichier Excel...\")\n",
    "\n",
    "    # Colonnes supplémentaires pour les 3 métriques demandées\n",
    "    table_cols = (\n",
    "        [\"Run Index\"] + list(grid_keys) +\n",
    "        [\"time_grooming\", \"time_rigid\", \"time_optimization\", \"time_total\",\n",
    "         \"compactness_95\", \"final_specificity_error\", \"final_generalization_error\"]\n",
    "    )\n",
    "\n",
    "    data_for_df = []\n",
    "\n",
    "    for i, res in enumerate(results, start=1):\n",
    "        row_dict = {}\n",
    "        row_dict[\"Run Index\"] = i\n",
    "\n",
    "        # Paramètres variables\n",
    "        for k in grid_keys:\n",
    "            row_dict[k] = res[\"params\"].get(k, None)\n",
    "\n",
    "        # On n'a groomé qu'une seule fois ; la pipeline d'optim n'a pas de grooming/rigid\n",
    "        # => On peut mettre 0 pour grooming, rigid,\n",
    "        #    ou si vous aviez mesuré ces temps, vous pourriez les stocker. \n",
    "        #    Ici on laisse 0. \n",
    "        st = res[\"step_times\"]\n",
    "        row_dict[\"time_grooming\"]      = \"0:00\"\n",
    "        row_dict[\"time_rigid\"]         = \"0:00\"\n",
    "        row_dict[\"time_optimization\"]  = mm_ss_format(st.get(\"optimization\", 0))\n",
    "        row_dict[\"time_total\"]         = mm_ss_format(res.get(\"total_execution_time\", 0))\n",
    "\n",
    "        # Ajout des 3 métriques dans la feuille \"PARAMS\"\n",
    "        # (on prend la dernière valeur en specificity / generalization)\n",
    "        met = res[\"metrics\"]\n",
    "        row_dict[\"compactness_95\"]            = met[\"compactness_required\"]\n",
    "        row_dict[\"final_specificity_error\"]   = ( met[\"specificity\"][-1] \n",
    "                                                  if len(met[\"specificity\"])>0 else None )\n",
    "        row_dict[\"final_generalization_error\"]= ( met[\"generalization\"][-1] \n",
    "                                                  if len(met[\"generalization\"])>0 else None )\n",
    "\n",
    "        data_for_df.append(row_dict)\n",
    "\n",
    "    # DataFrame final\n",
    "    params_df = pd.DataFrame(data_for_df, columns=table_cols)\n",
    "\n",
    "    # Ouverture du writer\n",
    "    writer = pd.ExcelWriter(excel_filename, engine='openpyxl')\n",
    "    params_df.to_excel(writer, sheet_name=\"PARAMS\", index=False)\n",
    "\n",
    "    # Ajustement colonnes\n",
    "    ws_params = writer.book[\"PARAMS\"]\n",
    "    col_widths = {\n",
    "        \"A\":15, \"B\":15, \"C\":15, \"D\":15, \"E\":15,\n",
    "        \"F\":15, \"G\":15, \"H\":15, \"I\":15, \"J\":20, \"K\":25, \"L\":25\n",
    "    }\n",
    "    for col, w in col_widths.items():\n",
    "        ws_params.column_dimensions[col].width = w\n",
    "\n",
    "    # Ajout du bloc paramètre complet en I–J (ligne = len(results)+3)\n",
    "    row_for_params = len(results) + 3\n",
    "\n",
    "    grooming_keys = [\n",
    "        \"ANTIALIAS_ITERATIONS\", \"ISO_SPACING\", \"PAD_SIZE\",\n",
    "        \"PAD_VALUE\", \"ISO_VALUE\", \"ICP_ITERATIONS\"\n",
    "    ]\n",
    "    all_param_names = sorted(set(OPT_PARAMS.keys()) | set(grooming_keys))\n",
    "\n",
    "    param_names_list  = []\n",
    "    param_values_list = []\n",
    "    for pname in all_param_names:\n",
    "        if pname in OPT_PARAMS:\n",
    "            val = OPT_PARAMS[pname]\n",
    "        else:\n",
    "            if pname == \"ANTIALIAS_ITERATIONS\":\n",
    "                val = ANTIALIAS_ITERATIONS\n",
    "            elif pname == \"ISO_SPACING\":\n",
    "                val = str(ISO_SPACING)\n",
    "            elif pname == \"PAD_SIZE\":\n",
    "                val = PAD_SIZE\n",
    "            elif pname == \"PAD_VALUE\":\n",
    "                val = PAD_VALUE\n",
    "            elif pname == \"ISO_VALUE\":\n",
    "                val = ISO_VALUE\n",
    "            elif pname == \"ICP_ITERATIONS\":\n",
    "                val = ICP_ITERATIONS\n",
    "            else:\n",
    "                val = None\n",
    "        param_names_list.append(str(pname))\n",
    "        param_values_list.append(str(val))\n",
    "\n",
    "    c_names  = ws_params.cell(row=row_for_params, column=10)  # J\n",
    "    c_values = ws_params.cell(row=row_for_params, column=11)  # K\n",
    "\n",
    "    c_names.value  = \"\\n\".join(param_names_list)\n",
    "    c_values.value = \"\\n\".join(param_values_list)\n",
    "\n",
    "    c_names.alignment  = Alignment(wrap_text=True)\n",
    "    c_values.alignment = Alignment(wrap_text=True)\n",
    "\n",
    "\n",
    "    # Feuilles \"Run_i\" (graphes + PC1/PC2)\n",
    "    for i, res in enumerate(results, start=1):\n",
    "        sheet_name = f\"Run_{i}\"\n",
    "        df_dummy = pd.DataFrame()\n",
    "        df_dummy.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        ws_run = writer.book[sheet_name]\n",
    "\n",
    "        plot_dir = os.path.join(BASE_OUTPUT_DIR, f\"Run_{i}\", \"plots\")\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        met = res[\"metrics\"]\n",
    "        cvar = met[\"cumulative_variance\"]\n",
    "        specificity_data = met[\"specificity\"]\n",
    "        general_data = met[\"generalization\"]\n",
    "\n",
    "        compactness_img = _plot_metric_curve(cvar, \"Compactness\", \"Variance\", i, plot_dir)\n",
    "        specificity_img = _plot_metric_curve(specificity_data, \"Specificity Error\", \"Error\", i, plot_dir)\n",
    "        general_img     = _plot_metric_curve(general_data, \"Generalization Error\", \"Error\", i, plot_dir)\n",
    "\n",
    "        row_img1 = 1\n",
    "        row_img2 = 12\n",
    "        row_img3 = 23\n",
    "\n",
    "        # Insertion d'images\n",
    "        imgA = ExcelImage(compactness_img); imgA.width, imgA.height = 310, 180\n",
    "        imgB = ExcelImage(specificity_img); imgB.width, imgB.height = 310, 180\n",
    "        imgC = ExcelImage(general_img);     imgC.width, imgC.height = 310, 180\n",
    "\n",
    "        ws_run.add_image(imgA, f\"A{row_img1}\")\n",
    "        ws_run.add_image(imgB, f\"A{row_img2}\")\n",
    "        ws_run.add_image(imgC, f\"A{row_img3}\")\n",
    "\n",
    "        # Petits commentaires\n",
    "        ws_run.cell(row=row_img1+9, column=1, value=f\"Composantes pour 95%: {met['compactness_required']}\")\n",
    "        if len(specificity_data) > 0:\n",
    "            ws_run.cell(row=row_img2+9, column=1, value=f\"Specificity final error: {specificity_data[-1]:.4f}\")\n",
    "        if len(general_data) > 0:\n",
    "            ws_run.cell(row=row_img3+9, column=1, value=f\"Generalization final error: {general_data[-1]:.4f}\")\n",
    "\n",
    "        # PC1/PC2\n",
    "        pc1_pc2_df = pd.DataFrame(res[\"pca_projection\"], columns=[\"PC1\", \"PC2\"])\n",
    "        pc1_pc2_df.to_excel(writer, sheet_name=sheet_name, startrow=34, index=False)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Fichier Excel sauvegardé : {excel_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5. Script Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Step 1. Acquire Data\n",
      "Nombre de shapes : 4\n",
      "\n",
      "----------------------------------------\n",
      "Step 2. Groom - Data Pre-processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grooming shapes: 100%|██████████| 4/4 [01:31<00:00, 22.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Step 3. Groom - Rigid Transformations\n",
      "Image de référence trouvée : RF_S2_FULGUR_008_181477_label_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul des transformations: 100%|██████████| 4/4 [01:25<00:00, 21.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Exécution 1 avec les paramètres :\n",
      "  number_of_particles = 16\n",
      "\n",
      "========================================\n",
      "Lancement Optim+Analyse - Exécution 1\n",
      "\n",
      "----------------------------------------\n",
      "Step 4. Optimize - Particle Based Optimization\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_007_48871_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_008_181477_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_009_25705_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_076_46061_left_label_4.vtk\n",
      "Lancement de l'optimisation via shapeworks...\n",
      "\n",
      "----------------------------------------\n",
      "Calcul de l'ACP\n",
      "Forme des particules aplaties : (4, 48)\n",
      "Résultats de la PCA sauvegardés dans c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\Run_1\\OUTPUT\\PCA_results\n",
      "\n",
      "----------------------------------------\n",
      "Calcul des métriques d'erreur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul de la spécificité: 100%|██████████| 3/3 [00:00<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution 1 terminée en 44.28 secondes\n",
      "\n",
      "========================================\n",
      "Exécution 2 avec les paramètres :\n",
      "  number_of_particles = 32\n",
      "\n",
      "========================================\n",
      "Lancement Optim+Analyse - Exécution 2\n",
      "\n",
      "----------------------------------------\n",
      "Step 4. Optimize - Particle Based Optimization\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_007_48871_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_008_181477_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_009_25705_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_076_46061_left_label_4.vtk\n",
      "Lancement de l'optimisation via shapeworks...\n",
      "\n",
      "----------------------------------------\n",
      "Calcul de l'ACP\n",
      "Forme des particules aplaties : (4, 96)\n",
      "Résultats de la PCA sauvegardés dans c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\Run_2\\OUTPUT\\PCA_results\n",
      "\n",
      "----------------------------------------\n",
      "Calcul des métriques d'erreur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul de la spécificité: 100%|██████████| 3/3 [00:00<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution 2 terminée en 31.26 secondes\n",
      "\n",
      "========================================\n",
      "Exécution 3 avec les paramètres :\n",
      "  number_of_particles = 64\n",
      "\n",
      "========================================\n",
      "Lancement Optim+Analyse - Exécution 3\n",
      "\n",
      "----------------------------------------\n",
      "Step 4. Optimize - Particle Based Optimization\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_007_48871_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_008_181477_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_009_25705_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_076_46061_left_label_4.vtk\n",
      "Lancement de l'optimisation via shapeworks...\n",
      "\n",
      "----------------------------------------\n",
      "Calcul de l'ACP\n",
      "Forme des particules aplaties : (4, 192)\n",
      "Résultats de la PCA sauvegardés dans c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\Run_3\\OUTPUT\\PCA_results\n",
      "\n",
      "----------------------------------------\n",
      "Calcul des métriques d'erreur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul de la spécificité: 100%|██████████| 3/3 [00:00<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution 3 terminée en 80.49 secondes\n",
      "\n",
      "========================================\n",
      "Exécution 4 avec les paramètres :\n",
      "  number_of_particles = 128\n",
      "\n",
      "========================================\n",
      "Lancement Optim+Analyse - Exécution 4\n",
      "\n",
      "----------------------------------------\n",
      "Step 4. Optimize - Particle Based Optimization\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_007_48871_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_008_181477_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_009_25705_label_4.vtk\n",
      "Writing: c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\GROOMING\\OUTPUT\\groomed\\meshes\\RF_S2_FULGUR_076_46061_left_label_4.vtk\n",
      "Lancement de l'optimisation via shapeworks...\n",
      "\n",
      "----------------------------------------\n",
      "Calcul de l'ACP\n",
      "Forme des particules aplaties : (4, 384)\n",
      "Résultats de la PCA sauvegardés dans c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\Run_4\\OUTPUT\\PCA_results\n",
      "\n",
      "----------------------------------------\n",
      "Calcul des métriques d'erreur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul de la spécificité: 100%|██████████| 3/3 [00:00<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exécution 4 terminée en 188.10 secondes\n",
      "\n",
      "Sauvegarde des résultats dans le fichier Excel...\n",
      "Fichier Excel sauvegardé : c:\\Users\\sacha\\Desktop\\ECN\\0_PROJ_REDEV\\SOURCE\\CODE\\OUTPUT_PIPELINE\\grid_search_results.xlsx\n",
      "\n",
      "Pipeline terminée.\n"
     ]
    }
   ],
   "source": [
    "# 1) Dossier pour grooming\n",
    "base_output_for_grooming = os.path.join(BASE_OUTPUT_DIR, \"GROOMING\")\n",
    "os.makedirs(base_output_for_grooming, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH_GROOM = os.path.join(base_output_for_grooming, \"OUTPUT\")\n",
    "os.makedirs(OUTPUT_PATH_GROOM, exist_ok=True)\n",
    "groom_dir = os.path.join(OUTPUT_PATH_GROOM, \"groomed\")\n",
    "\n",
    "# 2) Pré-traitement (grooming)\n",
    "(shape_seg_list,\n",
    " shape_filenames,\n",
    " dataset_ids,\n",
    " shape_names,\n",
    " rigid_transforms,\n",
    " groomed_files) = run_preprocessing(DATASET_PATHS, SHAPE_EXT, base_output_for_grooming)\n",
    "\n",
    "# 3) Liste des runs (combinaisons GRID_PARAMS)\n",
    "grid_keys = list(GRID_PARAMS.keys())\n",
    "grid_values = [GRID_PARAMS[k] for k in grid_keys]\n",
    "runs = []\n",
    "for combo in itertools.product(*grid_values):\n",
    "    run_dict = {}\n",
    "    for k, val in zip(grid_keys, combo):\n",
    "        run_dict[k] = val\n",
    "    runs.append(run_dict)\n",
    "\n",
    "# 4) Exécution en boucle\n",
    "results = []\n",
    "for i, run_params in enumerate(runs, start=1):\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"Exécution {i} avec les paramètres :\")\n",
    "    for key, val in run_params.items():\n",
    "        print(f\"  {key} = {val}\")\n",
    "\n",
    "    out = run_optimization_and_analysis(\n",
    "        run_params,\n",
    "        i,\n",
    "        shape_seg_list,\n",
    "        shape_filenames,\n",
    "        rigid_transforms,\n",
    "        groomed_files,\n",
    "        BASE_OUTPUT_DIR\n",
    "    )\n",
    "    results.append(out)\n",
    "\n",
    "# 5) Export Excel\n",
    "excel_filename = os.path.join(BASE_OUTPUT_DIR, \"grid_search_results.xlsx\")\n",
    "save_results_to_excel(results, excel_filename, grid_keys)\n",
    "\n",
    "print(\"\\nPipeline terminée.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
