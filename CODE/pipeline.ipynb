{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import shapeworks as sw\n",
    "\n",
    "# Pour tracer et insérer des images\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "import tempfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 0. Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Dossiers ----------\n",
    "DATASET_NAME       = \"TEST\"\n",
    "DATASET_PATHS      = [\n",
    "    #(os.path.join(\".\", \"DATA\", \"RF_FULGUR_M\"), \"RF\"),\n",
    "    (os.path.join(\".\", \"DATA\", \"RF_FULGUR_SAMPLE_2\"), \"RF_S2\") #pour tester\n",
    "]\n",
    "SHAPE_EXT          = \".nii.gz\"\n",
    "\n",
    "# Les sorties seront placées dans \"./OUTPUT_PIPELINE\"\n",
    "BASE_OUTPUT_DIR    = os.path.abspath(os.path.join(\".\", \"OUTPUT_PIPELINE\"))\n",
    "OPTION_SET         = \"default\"  # Option set à utiliser dans les noms de fichiers\n",
    "\n",
    "# ---------- Paramètres Grooming ----------\n",
    "ANTIALIAS_ITERATIONS = 30\n",
    "ISO_SPACING          = [1, 1, 1]\n",
    "PAD_SIZE             = 10     \n",
    "PAD_VALUE            = 0\n",
    "ISO_VALUE            = 0.5\n",
    "ICP_ITERATIONS       = 100\n",
    "\n",
    "# ---------- Paramètres par Défaut ----------\n",
    "OPT_PARAMS = {\n",
    "    \"number_of_particles\":       128,\n",
    "    \"use_normals\":               0,\n",
    "    \"normals_strength\":          10.0,\n",
    "    \"checkpointing_interval\":    1000,\n",
    "    \"keep_checkpoints\":          0,\n",
    "    \"iterations_per_split\":      1000,\n",
    "    \"optimization_iterations\":   1000,\n",
    "    \"starting_regularization\":   10,\n",
    "    \"ending_regularization\":     1,\n",
    "    \"relative_weighting\":        1,\n",
    "    \"initial_relative_weighting\":0.05,\n",
    "    \"procrustes_interval\":       0,\n",
    "    \"procrustes_scaling\":        0,\n",
    "    \"save_init_splits\":          0,\n",
    "    \"verbosity\":                 0,\n",
    "    \"multiscale\":                1,\n",
    "    \"multiscale_particles\":      32,\n",
    "    \"tiny_test\":                 False,\n",
    "    \"use_single_scale\":          0,\n",
    "    \"mesh_mode\":                 True,\n",
    "    \"option_set\":                OPTION_SET\n",
    "}\n",
    "\n",
    "# ---------- Paramètres Variables (pour grid search) ----------\n",
    "GRID_PARAMS = {\n",
    "    \"number_of_particles\": [16, 32]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DB = os.path.abspath(os.path.join(\".\", \"OUTPUT_DB\"))\n",
    "if not os.path.exists(OUTPUT_DB):\n",
    "    os.makedirs(OUTPUT_DB)\n",
    "\n",
    "# Si le dossier de sortie existe, déplacer les fichiers Excel qu'il contient\n",
    "if os.path.exists(BASE_OUTPUT_DIR):\n",
    "    for root, dirs, files in os.walk(BASE_OUTPUT_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                source_file = os.path.join(root, file)\n",
    "                # Créer un nom unique en ajoutant le timestamp\n",
    "                new_filename = f\"{os.path.splitext(file)[0]}_{int(time.time())}.xlsx\"\n",
    "                dest_file = os.path.join(OUTPUT_DB, new_filename)\n",
    "                shutil.move(source_file, dest_file)\n",
    "    # Supprimer entièrement le dossier de sortie pour repartir d'une base propre\n",
    "    shutil.rmtree(BASE_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 1. Grooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_data(DATASET_PATHS, SHAPE_EXT, OUTPUT_PATH):\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 1. Acquire Data\")\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    shape_filenames = []  # Liste des fichiers\n",
    "    dataset_ids = []      # Identifiants associés\n",
    "    for data_path, dataset_id in DATASET_PATHS:\n",
    "        files = sorted(glob.glob(os.path.join(data_path, '*' + SHAPE_EXT)))\n",
    "        shape_filenames.extend(files)\n",
    "        dataset_ids.extend([dataset_id] * len(files))\n",
    "    print(f\"Nombre de shapes : {len(shape_filenames)}\")\n",
    "    return shape_filenames, dataset_ids\n",
    "\n",
    "def groom_shapes(shape_filenames, dataset_ids, groom_dir):\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 2. Groom - Data Pre-processing\")\n",
    "    if not os.path.exists(groom_dir):\n",
    "        os.makedirs(groom_dir)\n",
    "    shape_seg_list = []\n",
    "    shape_names = []\n",
    "    for i, shape_filename in enumerate(tqdm(shape_filenames, desc=\"Grooming shapes\")):\n",
    "        dataset_id = dataset_ids[i]\n",
    "        base_shape_name = os.path.basename(shape_filename).replace(SHAPE_EXT, '')\n",
    "        shape_name = f\"{dataset_id}_{base_shape_name}\"\n",
    "        shape_names.append(shape_name)\n",
    "        shape_seg = sw.Image(shape_filename)\n",
    "        shape_seg_list.append(shape_seg)\n",
    "        bounding_box = sw.ImageUtils.boundingBox([shape_seg], ISO_VALUE).pad(2)\n",
    "        shape_seg.crop(bounding_box)\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).resample(ISO_SPACING, sw.InterpolationType.Linear).binarize()\n",
    "        shape_seg.pad(PAD_SIZE, PAD_VALUE)\n",
    "    return shape_seg_list, shape_names\n",
    "\n",
    "def rigid_transformations(shape_seg_list, shape_names, groom_dir):\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 3. Groom - Rigid Transformations\")\n",
    "    print(\"Recherche de l'image de référence...\")\n",
    "    ref_index = sw.find_reference_image_index(shape_seg_list)\n",
    "    ref_seg = shape_seg_list[ref_index]\n",
    "    ref_name = shape_names[ref_index]\n",
    "    ref_filename = os.path.join(groom_dir, 'reference.nii.gz')\n",
    "    ref_seg.write(ref_filename)\n",
    "    print(\"Image de référence trouvée : \" + ref_name)\n",
    "    transform_dir = os.path.join(groom_dir, 'rigid_transforms')\n",
    "    if not os.path.exists(transform_dir):\n",
    "        os.makedirs(transform_dir)\n",
    "    rigid_transforms = []\n",
    "    for shape_seg, shape_name in tqdm(zip(shape_seg_list, shape_names), desc=\"Calcul des transformations\", total=len(shape_seg_list)):\n",
    "        rigid_transform = shape_seg.createRigidRegistrationTransform(ref_seg, ISO_VALUE, ICP_ITERATIONS)\n",
    "        rigid_transform = sw.utils.getVTKtransform(rigid_transform)\n",
    "        rigid_transforms.append(rigid_transform)\n",
    "        transform_filename = os.path.join(transform_dir, f'{shape_name}_to_{ref_name}_transform.txt')\n",
    "        np.savetxt(transform_filename, rigid_transform)\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).computeDT(0).gaussianBlur(1.5)\n",
    "    # Détermination du mode de sauvegarde (meshes ou distance_transforms)\n",
    "    if OPT_PARAMS.get(\"mesh_mode\", True):\n",
    "        output_subdir = 'meshes'\n",
    "        output_extension = '.vtk'\n",
    "    else:\n",
    "        output_subdir = 'distance_transforms'\n",
    "        output_extension = SHAPE_EXT\n",
    "    OUTPUT_DIR = os.path.join(groom_dir, output_subdir)\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "    groomed_files = []\n",
    "    for shape_seg, shape_name in zip(shape_seg_list, shape_names):\n",
    "        output_filename = os.path.join(OUTPUT_DIR, shape_name + output_extension)\n",
    "        shape_seg.write(output_filename)\n",
    "        groomed_files.append(output_filename)\n",
    "    return rigid_transforms, groomed_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2. Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_particles(shape_seg_list, shape_filenames, rigid_transforms, groomed_files, OUTPUT_PATH):\n",
    "    \"\"\"\n",
    "    Fonction d'optimisation qui crée un projet ShapeWorks et lance shapeworks optimize.\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Step 4. Optimize - Particle Based Optimization\")\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    # --- Étape clé : on récupère le domain_type et on ajuste groomed_files\n",
    "    domain_type, groomed_files = sw.data.get_optimize_input(\n",
    "        groomed_files,\n",
    "        OPT_PARAMS[\"mesh_mode\"]  # True si on veut des meshes, False sinon\n",
    "    )\n",
    "\n",
    "    # Construction des sujets (1 seul domain par sujet)\n",
    "    subjects = []\n",
    "    number_domains = 1\n",
    "    for i in range(len(shape_seg_list)):\n",
    "        subject = sw.Subject()\n",
    "        subject.set_number_of_domains(number_domains)\n",
    "        # Fichiers originaux\n",
    "        abs_shape_filename = os.path.abspath(shape_filenames[i])\n",
    "        subject.set_original_filenames([abs_shape_filename])\n",
    "        # Fichiers groomed\n",
    "        groomed_file = os.path.abspath(groomed_files[i])\n",
    "        subject.set_groomed_filenames([groomed_file])\n",
    "        # Transformations rigides\n",
    "        transform = [rigid_transforms[i].flatten()]\n",
    "        subject.set_groomed_transforms(transform)\n",
    "        # Pour versions récentes de ShapeWorks\n",
    "        try:\n",
    "            subject.set_domain_type(0, domain_type)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        subjects.append(subject)\n",
    "\n",
    "    # Création du projet ShapeWorks et ajout des paramètres\n",
    "    project = sw.Project()\n",
    "    project.set_subjects(subjects)\n",
    "    parameters = sw.Parameters()\n",
    "\n",
    "    # On prépare uniquement les paramètres valides pour l'optimisation\n",
    "    valid_params = {\n",
    "        \"number_of_particles\":        OPT_PARAMS[\"number_of_particles\"],\n",
    "        \"use_normals\":                OPT_PARAMS[\"use_normals\"],\n",
    "        \"normals_strength\":           OPT_PARAMS[\"normals_strength\"],\n",
    "        \"checkpointing_interval\":     OPT_PARAMS[\"checkpointing_interval\"],\n",
    "        \"keep_checkpoints\":           OPT_PARAMS[\"keep_checkpoints\"],\n",
    "        \"iterations_per_split\":       OPT_PARAMS[\"iterations_per_split\"],\n",
    "        \"optimization_iterations\":    OPT_PARAMS[\"optimization_iterations\"],\n",
    "        \"starting_regularization\":    OPT_PARAMS[\"starting_regularization\"],\n",
    "        \"ending_regularization\":      OPT_PARAMS[\"ending_regularization\"],\n",
    "        \"relative_weighting\":         OPT_PARAMS[\"relative_weighting\"],\n",
    "        \"initial_relative_weighting\": OPT_PARAMS[\"initial_relative_weighting\"],\n",
    "        \"procrustes_interval\":        OPT_PARAMS[\"procrustes_interval\"],\n",
    "        \"procrustes_scaling\":         OPT_PARAMS[\"procrustes_scaling\"],\n",
    "        \"save_init_splits\":           OPT_PARAMS[\"save_init_splits\"],\n",
    "        \"verbosity\":                  OPT_PARAMS[\"verbosity\"]\n",
    "    }\n",
    "\n",
    "    if OPT_PARAMS.get(\"tiny_test\", False):\n",
    "        valid_params[\"number_of_particles\"] = 32\n",
    "        valid_params[\"optimization_iterations\"] = 25\n",
    "\n",
    "    # Multiscale ou single scale\n",
    "    if not OPT_PARAMS.get(\"use_single_scale\", 0):\n",
    "        valid_params[\"multiscale\"] = 1\n",
    "        valid_params[\"multiscale_particles\"] = OPT_PARAMS[\"multiscale_particles\"]\n",
    "\n",
    "    # Remplir les paramètres\n",
    "    for key, val in valid_params.items():\n",
    "        parameters.set(key, sw.Variant([val]))\n",
    "    project.set_parameters(\"optimize\", parameters)\n",
    "\n",
    "    # Sauvegarde du fichier projet\n",
    "    proj_filename = os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_{OPTION_SET}.swproj\")\n",
    "    project.save(proj_filename)\n",
    "\n",
    "    # Lancement de l'optimisation\n",
    "    print(\"Lancement de l'optimisation via shapeworks...\")\n",
    "    optimize_cmd = ['shapeworks', 'optimize', '--progress', '--name', proj_filename]\n",
    "    subprocess.check_call(optimize_cmd, cwd=OUTPUT_PATH)\n",
    "\n",
    "    # Vérification finale\n",
    "    args_for_check = type('ArgsForCheck', (object,), {})()\n",
    "    args_for_check.tiny_test = OPT_PARAMS.get(\"tiny_test\", False)\n",
    "    args_for_check.verify    = False\n",
    "    sw.utils.check_results(args_for_check, proj_filename)\n",
    "\n",
    "    return os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_default_particles\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3. Analyse en Composantes Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_particles(particles_dir, particle_type=\"world\"):\n",
    "    particles = []\n",
    "    names = []\n",
    "    for filename in os.listdir(particles_dir):\n",
    "        if filename.endswith(particle_type + \".particles\"):\n",
    "            filepath = os.path.join(particles_dir, filename)\n",
    "            try:\n",
    "                data = np.loadtxt(filepath)\n",
    "                particles.append(data)\n",
    "                names.append(os.path.splitext(filename)[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur de lecture du fichier {filename}: {e}\")\n",
    "    if particles:\n",
    "        return np.array(particles), names\n",
    "    return None, None\n",
    "\n",
    "def compute_pca(particles_dir, PCA_OUTPUT_DIR):\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Calcul de l'ACP\")\n",
    "    if not os.path.exists(PCA_OUTPUT_DIR):\n",
    "        os.makedirs(PCA_OUTPUT_DIR)\n",
    "    particles, shape_names = get_particles(particles_dir, particle_type=\"world\")\n",
    "    if particles is None:\n",
    "        raise ValueError(\"Aucune particule chargée depuis \" + particles_dir)\n",
    "    particles_flat = particles.reshape(particles.shape[0], -1)\n",
    "    n, d = particles_flat.shape\n",
    "    print(f\"Forme des particules aplaties : {n} x {d}\")\n",
    "    pca = PCA(n_components=n - 1)\n",
    "    pca.fit(particles_flat)\n",
    "    components = pca.transform(particles_flat)\n",
    "    # Projection sur 2 composantes\n",
    "    pca_projection = components[:, :2]\n",
    "    eigenvalues_file = os.path.join(PCA_OUTPUT_DIR, 'eigenvalues.eval')\n",
    "    with open(eigenvalues_file, 'w') as f:\n",
    "        for eigenvalue in pca.explained_variance_:\n",
    "            f.write(f'{eigenvalue}\\n')\n",
    "    eigenvectors = pca.components_\n",
    "    eigenvectors_reshaped = eigenvectors.reshape(eigenvectors.shape[0], -1, 3)\n",
    "    for i, eigenvector in enumerate(eigenvectors_reshaped):\n",
    "        filename = os.path.join(PCA_OUTPUT_DIR, f'eigenvector_{i+1}.eig')\n",
    "        np.savetxt(filename, eigenvector, fmt='%f')\n",
    "    print(\"Résultats de la PCA sauvegardés dans \" + PCA_OUTPUT_DIR)\n",
    "    return pca_projection, pca.explained_variance_, eigenvalues_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 4. Métriques d'Erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compactness(eigenvalues, threshold=0.95):\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    cumulative_variance = np.cumsum(eigenvalues) / total_variance\n",
    "    num_components = int(np.argmax(cumulative_variance >= threshold) + 1)\n",
    "    return num_components, cumulative_variance\n",
    "\n",
    "def load_shapes(particles_dir, particle_type=\"world\"):\n",
    "    shapes = []\n",
    "    for filename in os.listdir(particles_dir):\n",
    "        if filename.endswith(particle_type + '.particles'):\n",
    "            filepath = os.path.join(particles_dir, filename)\n",
    "            shape = np.loadtxt(filepath)\n",
    "            shapes.append(shape)\n",
    "    return np.array(shapes)\n",
    "\n",
    "def compute_specificity(real_shapes, num_particles, num_samples=1000):\n",
    "    n, p, dim3 = real_shapes.shape\n",
    "    d = p * dim3\n",
    "    real_shapes = real_shapes.reshape(n, d)\n",
    "    Y = real_shapes.T\n",
    "    mu = np.mean(Y, axis=1, keepdims=True)\n",
    "    Y_centered = Y - mu\n",
    "    U, S, _ = np.linalg.svd(Y_centered, full_matrices=False)\n",
    "    if S[0] < S[-1]:\n",
    "        S = S[::-1]\n",
    "        U = np.fliplr(U)\n",
    "    specificities = np.zeros(n - 1)\n",
    "    training_data = Y.T\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "    for m in tqdm(range(1, n), desc=\"Calcul de la spécificité\"):\n",
    "        epsi = U[:, :m]\n",
    "        stdevs = np.sqrt(S[:m])\n",
    "        betas = np.random.randn(m, num_samples)\n",
    "        for i_mode in range(m):\n",
    "            betas[i_mode, :] *= stdevs[i_mode]\n",
    "        synthetic = epsi @ betas + mu\n",
    "        min_dists = np.zeros(num_samples)\n",
    "        for isyn in range(num_samples):\n",
    "            synth_i = synthetic[:, isyn]\n",
    "            best = 1e15\n",
    "            for j in range(n):\n",
    "                dist_ij = shape_distance(synth_i, training_data[j], num_particles)\n",
    "                if dist_ij < best:\n",
    "                    best = dist_ij\n",
    "            min_dists[isyn] = best\n",
    "        avg_min = np.mean(min_dists) / float(num_particles)\n",
    "        specificities[m-1] = avg_min\n",
    "    return specificities\n",
    "\n",
    "def compute_generalization(real_shapes, num_particles):\n",
    "    if len(real_shapes.shape) == 3 and real_shapes.shape[2] == 3:\n",
    "        n, p, dim3 = real_shapes.shape\n",
    "        d = p * dim3\n",
    "        real_shapes = real_shapes.reshape(n, d)\n",
    "    else:\n",
    "        n, d = real_shapes.shape\n",
    "    if d != 3 * num_particles:\n",
    "        raise ValueError(f\"d={d}, attendu 3*num_particles={3*num_particles}\")\n",
    "    P = real_shapes.T\n",
    "    generalizations = np.zeros(n - 1)\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "    for m in range(1, n):\n",
    "        total_dist = 0.0\n",
    "        for leave in range(n):\n",
    "            Y = np.zeros((d, n-1))\n",
    "            Y[:, :leave] = P[:, :leave]\n",
    "            Y[:, leave:] = P[:, leave+1:]\n",
    "            mu = np.mean(Y, axis=1, keepdims=True)\n",
    "            Y_centered = Y - mu\n",
    "            U, S, _ = np.linalg.svd(Y_centered, full_matrices=False)\n",
    "            epsi = U[:, :m]\n",
    "            y_test = P[:, leave:leave+1]\n",
    "            betas = epsi.T @ (y_test - mu)\n",
    "            rec = epsi @ betas + mu\n",
    "            dist = shape_distance(rec, y_test, num_particles) / float(num_particles)\n",
    "            total_dist += dist\n",
    "        generalizations[m - 1] = total_dist / float(n)\n",
    "    return generalizations\n",
    "\n",
    "def compute_error_metrics(particles_dir, PCA_OUTPUT_DIR, num_particles):\n",
    "    print(\"\\n----------------------------------------\")\n",
    "    print(\"Calcul des métriques d'erreur\")\n",
    "    real_shapes = load_shapes(particles_dir, \"world\")\n",
    "    eigenvalues = np.loadtxt(os.path.join(PCA_OUTPUT_DIR, 'eigenvalues.eval'))\n",
    "    req_comp, cum_variance = compute_compactness(eigenvalues)\n",
    "    specifics = compute_specificity(real_shapes.reshape(real_shapes.shape[0], -1, 3), num_particles)\n",
    "    generalizations = compute_generalization(real_shapes.reshape(real_shapes.shape[0], -1, 3), num_particles)\n",
    "    metrics = {\n",
    "        \"compactness_required\": req_comp,\n",
    "        \"cumulative_variance\": cum_variance.tolist(),\n",
    "        \"specificity\": specifics.tolist(),\n",
    "        \"generalization\": generalizations.tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 5. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(run_params, run_index):\n",
    "    \"\"\"\n",
    "    Pipeline complète :\n",
    "      - Acquisition des données\n",
    "      - Grooming\n",
    "      - Rigid\n",
    "      - Optimisation\n",
    "      - PCA\n",
    "      - Métriques\n",
    "    \"\"\"\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"Lancement de la pipeline - Exécution {run_index}\")\n",
    "    overall_start = time.time()\n",
    "\n",
    "    # Sorties\n",
    "    BASE_OUTPUT = os.path.join(BASE_OUTPUT_DIR, f\"Run_{run_index}\")\n",
    "    os.makedirs(BASE_OUTPUT, exist_ok=True)\n",
    "    OUTPUT_PATH = os.path.join(BASE_OUTPUT, \"OUTPUT\")\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    groom_dir = os.path.join(OUTPUT_PATH, \"groomed\")\n",
    "    PCA_OUTPUT_DIR = os.path.join(OUTPUT_PATH, \"PCA_results\")\n",
    "    \n",
    "    step_times = {}\n",
    "    \n",
    "    # 1: Acquisition\n",
    "    t0 = time.time()\n",
    "    shape_filenames, dataset_ids = acquire_data(DATASET_PATHS, SHAPE_EXT, OUTPUT_PATH)\n",
    "    step_times[\"acquisition\"] = time.time() - t0\n",
    "    \n",
    "    # 2: Grooming\n",
    "    t0 = time.time()\n",
    "    shape_seg_list, shape_names = groom_shapes(shape_filenames, dataset_ids, groom_dir)\n",
    "    step_times[\"grooming\"] = time.time() - t0\n",
    "    \n",
    "    # 3: Rigid\n",
    "    t0 = time.time()\n",
    "    rigid_transforms, groomed_files = rigid_transformations(shape_seg_list, shape_names, groom_dir)\n",
    "    step_times[\"rigid\"] = time.time() - t0\n",
    "    \n",
    "    # 4: Optimization\n",
    "    t0 = time.time()\n",
    "    OPT_PARAMS.update(run_params)\n",
    "    particles_dir = optimize_particles(shape_seg_list, shape_filenames, rigid_transforms, groomed_files, OUTPUT_PATH)\n",
    "    step_times[\"optimization\"] = time.time() - t0\n",
    "    \n",
    "    # 5: PCA\n",
    "    t0 = time.time()\n",
    "    pca_projection, eigenvalues, _ = compute_pca(particles_dir, PCA_OUTPUT_DIR)\n",
    "    step_times[\"pca\"] = time.time() - t0\n",
    "    \n",
    "    # 6: Error metrics\n",
    "    t0 = time.time()\n",
    "    num_particles = OPT_PARAMS.get(\"number_of_particles\", 128)\n",
    "    metrics = compute_error_metrics(particles_dir, PCA_OUTPUT_DIR, num_particles)\n",
    "    step_times[\"error_metrics\"] = time.time() - t0\n",
    "    \n",
    "    overall_time = time.time() - overall_start\n",
    "    print(f\"Exécution {run_index} terminée en {overall_time:.2f} secondes\")\n",
    "    \n",
    "    return {\n",
    "        \"pca_projection\": pca_projection,\n",
    "        \"metrics\": metrics,\n",
    "        \"params\": run_params,\n",
    "        \"step_times\": step_times,\n",
    "        \"total_execution_time\": overall_time\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 6. Génération du fichier Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_metric_curve(data, title, ylabel, run_idx, outdir, figsize=(5, 3)):\n",
    "    \"\"\"\n",
    "    Petite fonction utilitaire pour tracer et sauvegarder un graphique.\n",
    "    - data: valeurs à tracer\n",
    "    - title, ylabel: légendes du graphe\n",
    "    - run_idx: index du run (pour nommer l'image)\n",
    "    - outdir: répertoire de sauvegarde\n",
    "    - figsize: tuple (width, height) en pouces\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    filename = f\"{title.replace(' ','_')}_Run_{run_idx}.png\"\n",
    "    image_path = os.path.join(outdir, filename)\n",
    "\n",
    "    # Utilise figsize passé en paramètre\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(range(1, len(data) + 1), data, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Number of Modes\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.savefig(image_path, dpi=130, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_excel(results, excel_filename, grid_keys):\n",
    "    \"\"\"\n",
    "    Feuille \"PARAMS\" :\n",
    "      - Colonnes A–H : (Run Index, grid_keys, 4 temps)\n",
    "      - Largeur des colonnes A–G = 15, I = 30\n",
    "      - En dessous (ligne len(results)+3), colonnes I–J : \n",
    "        tous les paramètres OPT_PARAMS + grooming (multiligne).\n",
    "    \n",
    "    Feuille \"Run_i\" :\n",
    "      - Graphes (Compactness, Specificity, Generalization) aux lignes 1, 13, 25,\n",
    "      - Taille 280×168 px,\n",
    "      - Infos 8 lignes en dessous,\n",
    "      - PC1/PC2 à la ligne 36.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nSauvegarde des résultats dans le fichier Excel...\")\n",
    "\n",
    "    # 1) Tableau principal A–H : run index, grid keys, 4 temps\n",
    "    table_cols = [\"Run Index\"] + list(grid_keys) + [\n",
    "        \"time_grooming\", \"time_rigid\", \"time_optimization\", \"time_total\"\n",
    "    ]\n",
    "    data_for_df = []\n",
    "\n",
    "    for i, res in enumerate(results, start=1):\n",
    "        row_dict = {}\n",
    "        row_dict[\"Run Index\"] = i\n",
    "\n",
    "        # Paramètres variables\n",
    "        for k in grid_keys:\n",
    "            row_dict[k] = res[\"params\"].get(k, None)\n",
    "\n",
    "        # 4 temps en minutes arrondies\n",
    "        st = res.get(\"step_times\", {})\n",
    "        row_dict[\"time_grooming\"]     = round(st.get(\"grooming\", 0) / 60)\n",
    "        row_dict[\"time_rigid\"]        = round(st.get(\"rigid\", 0) / 60)\n",
    "        row_dict[\"time_optimization\"] = round(st.get(\"optimization\", 0) / 60)\n",
    "        row_dict[\"time_total\"]        = round(res.get(\"total_execution_time\", 0) / 60)\n",
    "\n",
    "        data_for_df.append(row_dict)\n",
    "\n",
    "    params_df = pd.DataFrame(data_for_df, columns=table_cols)\n",
    "\n",
    "    # 2) Liste de tous les paramètres\n",
    "    grooming_keys = [\n",
    "        \"ANTIALIAS_ITERATIONS\", \"ISO_SPACING\", \"PAD_SIZE\",\n",
    "        \"PAD_VALUE\", \"ISO_VALUE\", \"ICP_ITERATIONS\"\n",
    "    ]\n",
    "    all_param_names = sorted(set(OPT_PARAMS.keys()) | set(grooming_keys))\n",
    "\n",
    "    # 3) Écriture du DataFrame dans \"PARAMS\"\n",
    "    writer = pd.ExcelWriter(excel_filename, engine='openpyxl')\n",
    "    params_df.to_excel(writer, sheet_name=\"PARAMS\", index=False)\n",
    "\n",
    "    ws_params = writer.book[\"PARAMS\"]\n",
    "\n",
    "    # --- Ajuster la largeur des colonnes ---\n",
    "    for col_letter in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"]:\n",
    "        ws_params.column_dimensions[col_letter].width = 15\n",
    "    # Colonne H n'est pas explicitement mentionnée, on peut lui laisser la largeur par défaut\n",
    "    # ou la mettre aussi à 15 si souhaité :\n",
    "    ws_params.column_dimensions[\"H\"].width = 15\n",
    "    ws_params.column_dimensions[\"I\"].width = 30\n",
    "    # La J reste par défaut\n",
    "\n",
    "    # 4) Insérer dans la feuille \"PARAMS\", en I–J (ligne = len(results)+3),\n",
    "    #    tous les paramètres en multiligne (noms / valeurs)\n",
    "    from openpyxl.styles import Alignment\n",
    "\n",
    "    row_params = len(results) + 3\n",
    "    param_names_list  = []\n",
    "    param_values_list = []\n",
    "\n",
    "    for pname in all_param_names:\n",
    "        if pname in OPT_PARAMS:\n",
    "            val = OPT_PARAMS[pname]\n",
    "        else:\n",
    "            if pname == \"ANTIALIAS_ITERATIONS\":\n",
    "                val = ANTIALIAS_ITERATIONS\n",
    "            elif pname == \"ISO_SPACING\":\n",
    "                val = str(ISO_SPACING)\n",
    "            elif pname == \"PAD_SIZE\":\n",
    "                val = PAD_SIZE\n",
    "            elif pname == \"PAD_VALUE\":\n",
    "                val = PAD_VALUE\n",
    "            elif pname == \"ISO_VALUE\":\n",
    "                val = ISO_VALUE\n",
    "            elif pname == \"ICP_ITERATIONS\":\n",
    "                val = ICP_ITERATIONS\n",
    "            else:\n",
    "                val = None\n",
    "\n",
    "        param_names_list.append(str(pname))\n",
    "        param_values_list.append(str(val))\n",
    "\n",
    "    multiline_names  = \"\\n\".join(param_names_list)\n",
    "    multiline_values = \"\\n\".join(param_values_list)\n",
    "\n",
    "    c_names  = ws_params.cell(row=row_params, column=9)   # I\n",
    "    c_values = ws_params.cell(row=row_params, column=10)  # J\n",
    "\n",
    "    c_names.value  = multiline_names\n",
    "    c_values.value = multiline_values\n",
    "\n",
    "    c_names.alignment  = Alignment(wrap_text=True)\n",
    "    c_values.alignment = Alignment(wrap_text=True)\n",
    "\n",
    "    # 5) Feuilles \"Run_i\"\n",
    "    for i, res in enumerate(results, start=1):\n",
    "        sheet_name = f\"Run_{i}\"\n",
    "        dummy_df = pd.DataFrame()\n",
    "        dummy_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        ws_run = writer.book[sheet_name]\n",
    "\n",
    "        # Graphes\n",
    "        plot_dir = os.path.join(BASE_OUTPUT_DIR, f\"Run_{i}\", \"plots\")\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        cvar = res[\"metrics\"][\"cumulative_variance\"]\n",
    "        specificity_data = res[\"metrics\"][\"specificity\"]\n",
    "        general_data = res[\"metrics\"][\"generalization\"]\n",
    "\n",
    "        compactness_img = _plot_metric_curve(cvar, \"Compactness\", \"Variance\", i, plot_dir)\n",
    "        specificity_img = _plot_metric_curve(specificity_data, \"Specificity Error\", \"Error\", i, plot_dir)\n",
    "        general_img     = _plot_metric_curve(general_data, \"Generalization Error\", \"Error\", i, plot_dir)\n",
    "\n",
    "        from openpyxl.drawing.image import Image as ExcelImage\n",
    "\n",
    "        # Placement plus serré\n",
    "        row_img1 = 1\n",
    "        row_img2 = 12\n",
    "        row_img3 = 23\n",
    "\n",
    "        # Agrandir de 40 % => 280×168\n",
    "        imgA = ExcelImage(compactness_img)\n",
    "        imgA.width, imgA.height = 310, 180\n",
    "\n",
    "        imgB = ExcelImage(specificity_img)\n",
    "        imgB.width, imgB.height = 310, 180\n",
    "\n",
    "        imgC = ExcelImage(general_img)\n",
    "        imgC.width, imgC.height = 310, 180\n",
    "\n",
    "        ws_run.add_image(imgA, f\"A{row_img1}\")\n",
    "        ws_run.add_image(imgB, f\"A{row_img2}\")\n",
    "        ws_run.add_image(imgC, f\"A{row_img3}\")\n",
    "\n",
    "        comp_req = res[\"metrics\"][\"compactness_required\"]\n",
    "        ws_run.cell(row=row_img1+9, column=1, value=f\"Composantes pour 95%: {comp_req}\")\n",
    "\n",
    "        if len(specificity_data) > 0:\n",
    "            spec_final = specificity_data[-1]\n",
    "            ws_run.cell(row=row_img2+9, column=1,\n",
    "                        value=f\"Specificity final error: {spec_final:.4f}\")\n",
    "\n",
    "        if len(general_data) > 0:\n",
    "            gen_final = general_data[-1]\n",
    "            ws_run.cell(row=row_img3+9, column=1,\n",
    "                        value=f\"Generalization final error: {gen_final:.4f}\")\n",
    "\n",
    "        # PC1/PC2 à la ligne 36\n",
    "        pc1_pc2_df = pd.DataFrame(res[\"pca_projection\"], columns=[\"PC1\", \"PC2\"])\n",
    "        pc1_pc2_df.to_excel(writer, sheet_name=sheet_name, startrow=34, index=False)\n",
    "\n",
    "    # Sauvegarde\n",
    "    writer.close()\n",
    "    print(f\"Fichier Excel sauvegardé : {excel_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 7. Fonction Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_keys = list(GRID_PARAMS.keys())\n",
    "grid_values = [GRID_PARAMS[k] for k in grid_keys]\n",
    "runs = []\n",
    "\n",
    "for combination in itertools.product(*grid_values):\n",
    "    run_param = {}\n",
    "    for key, value in zip(grid_keys, combination):\n",
    "        run_param[key] = value\n",
    "    runs.append(run_param)\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, run in enumerate(runs, start=1):\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"Exécution avec les paramètres :\")\n",
    "    for key, value in run.items():\n",
    "        print(f\"  {key} = {value}\")\n",
    "    res = run_pipeline(run, i)\n",
    "    results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_filename = os.path.join(BASE_OUTPUT_DIR, \"grid_search_results.xlsx\")\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "save_results_to_excel(results, excel_filename, grid_keys)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapeworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
