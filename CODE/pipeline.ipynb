{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Création du fichier Excel\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") \n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl.drawing.image import Image as ExcelImage\n",
    "from openpyxl.styles import Alignment\n",
    "import openpyxl\n",
    "\n",
    "# Couleurs du Terminal\n",
    "from colorama import Fore, Style, init as colorama_init\n",
    "colorama_init(autoreset=True)\n",
    "\n",
    "# ShapeWorks \n",
    "import shapeworks as sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paramètres Généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME       = \"TEST\"\n",
    "DATASET_PATHS      = [\n",
    "        # KEEP THIS COMMENTS\n",
    "        # ('./DATA/RF_FULGUR_M', 'RF'),\n",
    "        # ('./DATA/RF_FULGUR_PRED', 'RFP'),\n",
    "        # ('./DATA/RF_DIASEM', 'RFDIA'),\n",
    "        # ('./DATA/RF_FULGUR_SAMPLE', 'TEST1'),\n",
    "        ('./DATA/RF_FULGUR_SAMPLE_2', 'TEST2'),\n",
    "        # ('./DATA/RF_DIASEM_SAMPLE', 'TESTDIA'),\n",
    "]\n",
    "\n",
    "# --- Paramètres Variables --- #\n",
    "\n",
    "GRID_OPTIMIZATION = {\n",
    "    \"number_of_particles\": [16, 32]\n",
    "}\n",
    "\n",
    "GRID_GROOMING = {\n",
    "    \"ICP_ITERATIONS\": [50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paramètres par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paramètres de Grooming par défaut ---\n",
    "ANTIALIAS_ITERATIONS = 30\n",
    "ISO_SPACING          = [1, 1, 1]\n",
    "PAD_SIZE             = 10\n",
    "PAD_VALUE            = 0\n",
    "ISO_VALUE            = 0.5\n",
    "ICP_ITERATIONS       = 100\n",
    "\n",
    "# --- Paramètres d'Optimisation par défaut ---\n",
    "OPT_PARAMS = {\n",
    "    \"number_of_particles\":       128,\n",
    "    \"use_normals\":               0,\n",
    "    \"normals_strength\":          10.0,\n",
    "    \"checkpointing_interval\":    1000,\n",
    "    \"keep_checkpoints\":          0,\n",
    "    \"iterations_per_split\":      1000,\n",
    "    \"optimization_iterations\":   1000,\n",
    "    \"starting_regularization\":   10,\n",
    "    \"ending_regularization\":     1,\n",
    "    \"relative_weighting\":        1,\n",
    "    \"initial_relative_weighting\":0.05,\n",
    "    \"procrustes_interval\":       0,\n",
    "    \"procrustes_scaling\":        0,\n",
    "    \"save_init_splits\":          0,\n",
    "    \"verbosity\":                 0,\n",
    "    \"multiscale\":                1,\n",
    "    \"multiscale_particles\":      32,\n",
    "    \"tiny_test\":                 False,\n",
    "    \"use_single_scale\":          0,\n",
    "    \"mesh_mode\":                 True\n",
    "}\n",
    "\n",
    "# --- Dossiers ---\n",
    "\n",
    "SHAPE_EXT          = \".nii.gz\"\n",
    "DT_EXT             = \".nrrd\"\n",
    "BASE_OUTPUT_DIR    = os.path.abspath(os.path.join(\".\", \"OUTPUT_PIPELINE\"))\n",
    "OUTPUT_DB = os.path.abspath(os.path.join(\".\", \"OUTPUT_DB\"))\n",
    "\n",
    "# Création du dossier ou se sauvegardent automatiquement les excel\n",
    "os.makedirs(OUTPUT_DB, exist_ok=True)\n",
    "\n",
    "# Si le dossier de sortie existe déjà:\n",
    "# 1- On déplace les fichiers .xlsx dans le dossier OUTPUT_DB\n",
    "# 2- On supprime tout le contenu du dossier de sortie\n",
    "if os.path.exists(BASE_OUTPUT_DIR):\n",
    "    for root, dirs, files in os.walk(BASE_OUTPUT_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                source_file = os.path.join(root, file)\n",
    "                new_filename = f\"{os.path.splitext(file)[0]}_{int(time.time())}.xlsx\"\n",
    "                dest_file = os.path.join(OUTPUT_DB, new_filename)\n",
    "                shutil.move(source_file, dest_file)\n",
    "    shutil.rmtree(BASE_OUTPUT_DIR)\n",
    "\n",
    "# Création du dossier de sortie\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_print(msg, color=Fore.CYAN, style=Style.NORMAL):\n",
    "    \"\"\" Print a colored message \"\"\"\n",
    "    print(color + style + msg + Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aquisition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_data(dataset_paths, shape_ext, output_path):\n",
    "    color_print(\"\\n--- Step 1. Acquire Data ---\", Fore.GREEN, Style.BRIGHT)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    shape_filenames = []\n",
    "    dataset_ids = []\n",
    "\n",
    "    for data_path, dataset_id in dataset_paths:\n",
    "        files = sorted(glob.glob(os.path.join(data_path, '*' + shape_ext)))\n",
    "        shape_filenames.extend(files)\n",
    "        dataset_ids.extend([dataset_id] * len(files))\n",
    "\n",
    "    color_print(f\"  Nombre de shapes : {len(shape_filenames)}\", Fore.YELLOW)\n",
    "    return shape_filenames, dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_particles(particles_dir, particle_type=\"world\"):\n",
    "    particles = []\n",
    "    names = []\n",
    "    for filename in os.listdir(particles_dir):\n",
    "        if filename.endswith(particle_type + \".particles\"):\n",
    "            data = np.loadtxt(os.path.join(particles_dir, filename))\n",
    "            particles.append(data)\n",
    "            names.append(os.path.splitext(filename)[0])\n",
    "    if not particles:\n",
    "        return None, None\n",
    "    return np.array(particles), names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (Acquisition, Grooming, Rigid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groom_shapes(shape_filenames, dataset_ids, groom_dir):\n",
    "    color_print(\"\\n--- Step 2. Groom - Data Pre-processing ---\", Fore.GREEN, Style.BRIGHT)\n",
    "    os.makedirs(groom_dir, exist_ok=True)\n",
    "    \n",
    "    start_groom = time.time()\n",
    "    \n",
    "    shape_seg_list = []\n",
    "    shape_names = []\n",
    "\n",
    "    for i, shape_filename in enumerate(tqdm(shape_filenames, desc=\"Grooming shapes\")):\n",
    "        dataset_id = dataset_ids[i]\n",
    "        base_shape_name = os.path.basename(shape_filename).replace(SHAPE_EXT, '')\n",
    "        shape_name = f\"{dataset_id}_{base_shape_name}\"\n",
    "        shape_names.append(shape_name)\n",
    "\n",
    "        shape_seg = sw.Image(shape_filename)\n",
    "        shape_seg_list.append(shape_seg)\n",
    "\n",
    "        bounding_box = sw.ImageUtils.boundingBox([shape_seg], ISO_VALUE).pad(2)\n",
    "        shape_seg.crop(bounding_box)\n",
    "\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).resample(ISO_SPACING, sw.InterpolationType.Linear).binarize()\n",
    "        shape_seg.pad(PAD_SIZE, PAD_VALUE)\n",
    "\n",
    "    groom_time = time.time() - start_groom\n",
    "    color_print(f\"  Grooming time: {groom_time:.2f}s\", Fore.YELLOW)\n",
    "\n",
    "    return shape_seg_list, shape_names, groom_time\n",
    "\n",
    "def rigid_transformations(shape_seg_list, shape_names, groom_dir):\n",
    "    color_print(\"\\n--- Step 3. Groom - Rigid Transformations ---\", Fore.GREEN, Style.BRIGHT)\n",
    "    os.makedirs(groom_dir, exist_ok=True)\n",
    "\n",
    "    start_rigid = time.time()\n",
    "\n",
    "    ref_index = sw.find_reference_image_index(shape_seg_list)\n",
    "    ref_seg = shape_seg_list[ref_index]\n",
    "    ref_name = shape_names[ref_index]\n",
    "\n",
    "    ref_filename = os.path.join(groom_dir, 'reference' + DT_EXT)\n",
    "    ref_seg.write(ref_filename)\n",
    "    color_print(f\"  Image de référence trouvée : {ref_name}\", Fore.YELLOW)\n",
    "\n",
    "    transform_dir = os.path.join(groom_dir, 'rigid_transforms')\n",
    "    os.makedirs(transform_dir, exist_ok=True)\n",
    "\n",
    "    rigid_transforms = []\n",
    "\n",
    "    for shape_seg, shape_name in tqdm(zip(shape_seg_list, shape_names),\n",
    "                                      desc=\"Calcul des transformations\",\n",
    "                                      total=len(shape_seg_list)):\n",
    "        rigid_transform = shape_seg.createRigidRegistrationTransform(ref_seg, ISO_VALUE, ICP_ITERATIONS)\n",
    "        rigid_transform = sw.utils.getVTKtransform(rigid_transform)\n",
    "        rigid_transforms.append(rigid_transform)\n",
    "\n",
    "        transform_filename = os.path.join(transform_dir, f'{shape_name}_to_{ref_name}_transform.txt')\n",
    "        np.savetxt(transform_filename, rigid_transform)\n",
    "\n",
    "        shape_seg.antialias(ANTIALIAS_ITERATIONS).computeDT(0).gaussianBlur(1.5)\n",
    "\n",
    "    output_subdir = 'distance_transforms'\n",
    "    output_dir = os.path.join(groom_dir, output_subdir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    groomed_files = []\n",
    "    for shape_seg, shape_name in zip(shape_seg_list, shape_names):\n",
    "        out_name = os.path.join(output_dir, shape_name + DT_EXT)\n",
    "        shape_seg.write(out_name)\n",
    "        groomed_files.append(out_name)\n",
    "\n",
    "    rigid_time = time.time() - start_rigid\n",
    "    color_print(f\"  Rigid time: {rigid_time:.2f}s\", Fore.YELLOW)\n",
    "\n",
    "    return rigid_transforms, groomed_files, rigid_time\n",
    "\n",
    "def run_preprocessing(dataset_paths, shape_ext, grooming_params, base_dir):\n",
    "    \"\"\"\n",
    "    Applique les grooming_params (PAD_SIZE, ICP_ITERATIONS, etc.) \n",
    "    => acquisition => grooming => rigid\n",
    "    Retourne tout + le temps grooming + le temps rigid\n",
    "    \"\"\"\n",
    "    global PAD_SIZE, ICP_ITERATIONS, ISO_SPACING, ANTIALIAS_ITERATIONS, PAD_VALUE, ISO_VALUE\n",
    "\n",
    "    # Mise à jour des variables globales\n",
    "    for k, v in grooming_params.items():\n",
    "        if k == \"PAD_SIZE\":\n",
    "            PAD_SIZE = v\n",
    "        elif k == \"ICP_ITERATIONS\":\n",
    "            ICP_ITERATIONS = v\n",
    "        elif k == \"ISO_SPACING\":\n",
    "            ISO_SPACING = v\n",
    "        elif k == \"ANTIALIAS_ITERATIONS\":\n",
    "            ANTIALIAS_ITERATIONS = v\n",
    "        elif k == \"PAD_VALUE\":\n",
    "            PAD_VALUE = v\n",
    "        elif k == \"ISO_VALUE\":\n",
    "            ISO_VALUE = v\n",
    "        else:\n",
    "            color_print(f\"[WARNING] Paramètre grooming inconnu: {k} = {v}\", Fore.RED)\n",
    "\n",
    "    output_path = os.path.join(base_dir, \"OUTPUT\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    groom_dir = os.path.join(output_path, \"groomed\")\n",
    "\n",
    "    shape_filenames, dataset_ids = acquire_data(dataset_paths, shape_ext, output_path)\n",
    "\n",
    "    # Groom\n",
    "    shape_seg_list, shape_names, t_groom = groom_shapes(shape_filenames, dataset_ids, groom_dir)\n",
    "\n",
    "    # Rigid\n",
    "    rigid_transforms, groomed_files, t_rigid = rigid_transformations(shape_seg_list, shape_names, groom_dir)\n",
    "\n",
    "    return (shape_seg_list, shape_filenames, dataset_ids, shape_names, \n",
    "            rigid_transforms, groomed_files, t_groom, t_rigid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimisation et Particules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_particles(shape_seg_list, shape_filenames, rigid_transforms, groomed_files, output_path):\n",
    "    color_print(\"\\n--- Step 4. Optimize - Particle Based Optimization ---\", Fore.GREEN, Style.BRIGHT)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    domain_type, groomed_files_out = sw.data.get_optimize_input(\n",
    "        groomed_files,\n",
    "        OPT_PARAMS[\"mesh_mode\"]\n",
    "    )\n",
    "\n",
    "    subjects = []\n",
    "    for i in range(len(shape_seg_list)):\n",
    "        subj = sw.Subject()\n",
    "        subj.set_number_of_domains(1)\n",
    "\n",
    "        subj.set_original_filenames([os.path.abspath(shape_filenames[i])])\n",
    "        subj.set_groomed_filenames([os.path.abspath(groomed_files_out[i])])\n",
    "        subj.set_groomed_transforms([rigid_transforms[i].flatten()])\n",
    "\n",
    "        try:\n",
    "            subj.set_domain_type(0, domain_type)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        subjects.append(subj)\n",
    "\n",
    "    project = sw.Project()\n",
    "    project.set_subjects(subjects)\n",
    "    parameters = sw.Parameters()\n",
    "\n",
    "    valid_params = {\n",
    "        \"number_of_particles\":        OPT_PARAMS[\"number_of_particles\"],\n",
    "        \"use_normals\":                OPT_PARAMS[\"use_normals\"],\n",
    "        \"normals_strength\":           OPT_PARAMS[\"normals_strength\"],\n",
    "        \"checkpointing_interval\":     OPT_PARAMS[\"checkpointing_interval\"],\n",
    "        \"keep_checkpoints\":           OPT_PARAMS[\"keep_checkpoints\"],\n",
    "        \"iterations_per_split\":       OPT_PARAMS[\"iterations_per_split\"],\n",
    "        \"optimization_iterations\":    OPT_PARAMS[\"optimization_iterations\"],\n",
    "        \"starting_regularization\":    OPT_PARAMS[\"starting_regularization\"],\n",
    "        \"ending_regularization\":      OPT_PARAMS[\"ending_regularization\"],\n",
    "        \"relative_weighting\":         OPT_PARAMS[\"relative_weighting\"],\n",
    "        \"initial_relative_weighting\": OPT_PARAMS[\"initial_relative_weighting\"],\n",
    "        \"procrustes_interval\":        OPT_PARAMS[\"procrustes_interval\"],\n",
    "        \"procrustes_scaling\":         OPT_PARAMS[\"procrustes_scaling\"],\n",
    "        \"save_init_splits\":           OPT_PARAMS[\"save_init_splits\"],\n",
    "        \"verbosity\":                  OPT_PARAMS[\"verbosity\"]\n",
    "    }\n",
    "\n",
    "    if OPT_PARAMS.get(\"tiny_test\", False):\n",
    "        valid_params[\"number_of_particles\"] = 32\n",
    "        valid_params[\"optimization_iterations\"] = 25\n",
    "\n",
    "    if not OPT_PARAMS.get(\"use_single_scale\", 0):\n",
    "        valid_params[\"multiscale\"] = 1\n",
    "        valid_params[\"multiscale_particles\"] = OPT_PARAMS[\"multiscale_particles\"]\n",
    "\n",
    "    for k, v in valid_params.items():\n",
    "        parameters.set(k, sw.Variant([v]))\n",
    "\n",
    "    project.set_parameters(\"optimize\", parameters)\n",
    "\n",
    "    proj_file = os.path.join(output_path, f\"{DATASET_NAME}.swproj\")\n",
    "    project.save(proj_file)\n",
    "\n",
    "    color_print(\"  Lancement de l'optimisation via shapeworks...\", Fore.YELLOW)\n",
    "    cmd = ['shapeworks', 'optimize', '--progress', '--name', proj_file]\n",
    "    subprocess.check_call(cmd, cwd=output_path)\n",
    "\n",
    "    args_for_check = type('ArgsForCheck', (object,), {})()\n",
    "    args_for_check.tiny_test = OPT_PARAMS.get(\"tiny_test\", False)\n",
    "    args_for_check.verify    = False\n",
    "    sw.utils.check_results(args_for_check, proj_file)\n",
    "\n",
    "    return os.path.join(output_path, f\"{DATASET_NAME}_particles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PCA et Métriques d'Erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(particles_dir, pca_output_dir):\n",
    "    color_print(\"\\n--- Step 5. PCA ---\", Fore.GREEN, Style.BRIGHT)\n",
    "    os.makedirs(pca_output_dir, exist_ok=True)\n",
    "\n",
    "    parts, names = get_particles(particles_dir, \"world\")\n",
    "    if parts is None:\n",
    "        raise ValueError(\"Aucune particule chargée depuis \" + particles_dir)\n",
    "\n",
    "    n, p, _ = parts.shape\n",
    "    parts_flat = parts.reshape(n, -1)\n",
    "    color_print(f\"  Forme des particules aplaties : {parts_flat.shape}\", Fore.YELLOW)\n",
    "\n",
    "    pca = PCA(n_components=n - 1)\n",
    "    pca.fit(parts_flat)\n",
    "    comps = pca.transform(parts_flat)\n",
    "\n",
    "    eigvals = pca.explained_variance_\n",
    "    with open(os.path.join(pca_output_dir, 'eigenvalues.eval'), 'w') as f:\n",
    "        for ev in eigvals:\n",
    "            f.write(f\"{ev}\\n\")\n",
    "\n",
    "    eigenvectors = pca.components_\n",
    "    eigenvectors_reshaped = eigenvectors.reshape(eigenvectors.shape[0], -1, 3)\n",
    "    for i, eigvec in enumerate(eigenvectors_reshaped):\n",
    "        fn = os.path.join(pca_output_dir, f\"eigenvector_{i+1}.eig\")\n",
    "        np.savetxt(fn, eigvec, fmt='%f')\n",
    "\n",
    "    pca_projection = comps[:, :2]\n",
    "    color_print(\"  PCA calculée et sauvegardée.\", Fore.YELLOW)\n",
    "\n",
    "    return pca_projection, eigvals, names\n",
    "\n",
    "def compute_compactness(eigenvalues, threshold=0.95):\n",
    "    total_var = np.sum(eigenvalues)\n",
    "    cum_var = np.cumsum(eigenvalues) / total_var\n",
    "    num_comp = int(np.argmax(cum_var >= threshold) + 1)\n",
    "    return num_comp, cum_var\n",
    "\n",
    "def compute_specificity(real_shapes, num_particles, num_samples=1000):\n",
    "    color_print(\"  Calcul Specificity...\", Fore.YELLOW)\n",
    "    n, p, dim3 = real_shapes.shape\n",
    "    d = p * dim3\n",
    "    real_shapes_2d = real_shapes.reshape(n, d)\n",
    "\n",
    "    Y = real_shapes_2d.T\n",
    "    mu = np.mean(Y, axis=1, keepdims=True)\n",
    "    Yc = Y - mu\n",
    "    U, S, _ = np.linalg.svd(Yc, full_matrices=False)\n",
    "    if S[0] < S[-1]:\n",
    "        S = S[::-1]\n",
    "        U = np.fliplr(U)\n",
    "\n",
    "    specifics = np.zeros(n - 1)\n",
    "\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "\n",
    "    for m in tqdm(range(1, n), desc=\"  Specificity modes\"):\n",
    "        epsi = U[:, :m]\n",
    "        stdevs = np.sqrt(S[:m])\n",
    "        betas = np.random.randn(m, num_samples)\n",
    "        for i_mode in range(m):\n",
    "            betas[i_mode, :] *= stdevs[i_mode]\n",
    "        synth = epsi @ betas + mu\n",
    "        min_dists = np.zeros(num_samples)\n",
    "        for isyn in range(num_samples):\n",
    "            sy = synth[:, isyn]\n",
    "            best = 1e15\n",
    "            for j in range(n):\n",
    "                dist_j = shape_distance(sy, Y[:, j], num_particles)\n",
    "                if dist_j < best:\n",
    "                    best = dist_j\n",
    "            min_dists[isyn] = best\n",
    "        specifics[m-1] = np.mean(min_dists) / float(num_particles)\n",
    "\n",
    "    return specifics\n",
    "\n",
    "def compute_generalization(real_shapes, num_particles):\n",
    "    color_print(\"  Calcul Generalization...\", Fore.YELLOW)\n",
    "    if len(real_shapes.shape) == 3 and real_shapes.shape[2] == 3:\n",
    "        n, p, dim3 = real_shapes.shape\n",
    "        d = p * dim3\n",
    "        real_shapes_2d = real_shapes.reshape(n, d)\n",
    "    else:\n",
    "        n, d = real_shapes.shape\n",
    "        real_shapes_2d = real_shapes\n",
    "\n",
    "    def shape_distance(ptsA, ptsB, pcount):\n",
    "        A3 = ptsA.reshape(pcount, 3)\n",
    "        B3 = ptsB.reshape(pcount, 3)\n",
    "        return np.linalg.norm(A3 - B3, axis=1).sum()\n",
    "\n",
    "    P = real_shapes_2d.T\n",
    "    gens = np.zeros(n - 1)\n",
    "\n",
    "    for m in range(1, n):\n",
    "        tot_dist = 0.0\n",
    "        for leave in range(n):\n",
    "            Y = np.zeros((P.shape[0], n-1))\n",
    "            Y[:, :leave] = P[:, :leave]\n",
    "            Y[:, leave:] = P[:, leave+1:]\n",
    "            mu = np.mean(Y, axis=1, keepdims=True)\n",
    "            Yc = Y - mu\n",
    "            U, S, _ = np.linalg.svd(Yc, full_matrices=False)\n",
    "            epsi = U[:, :m]\n",
    "\n",
    "            ytest = P[:, leave:leave+1]\n",
    "            betas = epsi.T @ (ytest - mu)\n",
    "            rec = epsi @ betas + mu\n",
    "\n",
    "            dist = shape_distance(rec, ytest, num_particles) / float(num_particles)\n",
    "            tot_dist += dist\n",
    "        gens[m - 1] = tot_dist / float(n)\n",
    "\n",
    "    return gens\n",
    "\n",
    "def compute_error_metrics(particles_dir, pca_output_dir, num_particles):\n",
    "    color_print(\"\\n--- Step 6. Metrics ---\", Fore.GREEN, style=\"\")\n",
    "    real_shapes, real_names = get_particles(particles_dir, \"world\")\n",
    "    if real_shapes.size == 0:\n",
    "        raise ValueError(f\"Aucune shape chargée dans {particles_dir}\")\n",
    "\n",
    "    eigenvalues_path = os.path.join(pca_output_dir, 'eigenvalues.eval')\n",
    "    if not os.path.exists(eigenvalues_path):\n",
    "        raise FileNotFoundError(\"Fichier eigenvalues.eval introuvable : \" + eigenvalues_path)\n",
    "    eigenvalues = np.loadtxt(eigenvalues_path)\n",
    "\n",
    "    c_required, c_variance = compute_compactness(eigenvalues)\n",
    "    specifics = compute_specificity(real_shapes, num_particles)\n",
    "    generals = compute_generalization(real_shapes, num_particles)\n",
    "\n",
    "    metrics = {\n",
    "        \"compactness_required\": c_required,\n",
    "        \"cumulative_variance\": c_variance.tolist(),\n",
    "        \"specificity\": specifics.tolist(),\n",
    "        \"generalization\": generals.tolist()\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. run_optimization_and_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization_and_analysis(run_params, run_index,\n",
    "                                  shape_seg_list, shape_filenames,\n",
    "                                  rigid_transforms, groomed_files,\n",
    "                                  base_output_dir):\n",
    "    \"\"\"\n",
    "    Steps 4,5,6 : Optimization, PCA, Metrics\n",
    "    \"\"\"\n",
    "    color_print(f\"\\n  >>> RUN {run_index} : Optim + Analyse <<<\", Fore.MAGENTA, style=\"\")\n",
    "\n",
    "    overall_start = time.time()\n",
    "\n",
    "    run_dir = os.path.join(base_output_dir, f\"Run_{run_index}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    output_path = os.path.join(run_dir, \"OUTPUT\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    pca_out = os.path.join(output_path, \"PCA_results\")\n",
    "\n",
    "    # Mise à jour des OPT_PARAMS\n",
    "    for k,v in run_params.items():\n",
    "        OPT_PARAMS[k] = v\n",
    "\n",
    "    step_times = {}\n",
    "\n",
    "    # Step 4\n",
    "    t0 = time.time()\n",
    "    particles_dir = optimize_particles(shape_seg_list, shape_filenames, rigid_transforms, groomed_files, output_path)\n",
    "    step_times[\"optimization\"] = time.time() - t0\n",
    "\n",
    "    # Step 5\n",
    "    t0 = time.time()\n",
    "    pca_projection, eigvals, shape_names_for_pca = compute_pca(particles_dir, pca_out)\n",
    "    step_times[\"pca\"] = time.time() - t0\n",
    "\n",
    "    # Step 6\n",
    "    t0 = time.time()\n",
    "    n_parts = OPT_PARAMS.get(\"number_of_particles\", 128)\n",
    "    metrics = compute_error_metrics(particles_dir, pca_out, n_parts)\n",
    "    step_times[\"error_metrics\"] = time.time() - t0\n",
    "\n",
    "    overall_time = time.time() - overall_start\n",
    "    color_print(f\"  RUN {run_index} terminé en {overall_time:.2f}s\", Fore.MAGENTA)\n",
    "\n",
    "    # On remet les OPT_PARAMS comme avant si besoin (pas forcément)\n",
    "    return {\n",
    "        \"pca_projection\": pca_projection,\n",
    "        \"pca_shape_names\": shape_names_for_pca,\n",
    "        \"metrics\": metrics,\n",
    "        \"params\": run_params,\n",
    "        \"step_times\": step_times,\n",
    "        \"total_execution_time\": overall_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Création du Fichier Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_ss_format(seconds):\n",
    "    mm = int(seconds // 60)\n",
    "    ss = int(seconds % 60)\n",
    "    return f\"{mm}:{ss:02d}\"\n",
    "\n",
    "def _plot_metric_curve(data, title, ylabel, run_idx, outdir, figsize=(5, 3)):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    fn = f\"{title.replace(' ', '_')}_Run_{run_idx}.png\"\n",
    "    image_path = os.path.join(outdir, fn)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot(range(1, len(data) + 1), data, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Number of Modes\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.savefig(image_path, dpi=130, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image_path\n",
    "\n",
    "def _plot_pca_scatter(pc, shape_names, run_idx, outdir):\n",
    "    \"\"\"\n",
    "    Scatter plot (PC1 vs PC2), indexés 1..N + tableau associant index=>shape_name\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    fn_img = f\"PCA_Scatter_Run_{run_idx}.png\"\n",
    "    image_path = os.path.join(outdir, fn_img)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    x = pc[:,0]\n",
    "    y = pc[:,1]\n",
    "    ax.scatter(x, y, s=30, c='blue')\n",
    "\n",
    "    for i, (xx, yy) in enumerate(zip(x,y)):\n",
    "        ax.text(xx, yy, str(i+1), fontsize=8, color='red')\n",
    "\n",
    "    ax.set_title(\"PC1 vs PC2 Scatter\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_path, dpi=130)\n",
    "    plt.close(fig)\n",
    "\n",
    "    df_map = pd.DataFrame({\n",
    "        \"Index\": list(range(1, len(shape_names)+1)),\n",
    "        \"ShapeName\": shape_names\n",
    "    })\n",
    "\n",
    "    return image_path, df_map\n",
    "\n",
    "\n",
    "def save_results_to_excel(all_results, excel_filename, grooming_keys, optimization_keys):\n",
    "\n",
    "    color_print(\"\\nSauvegarde des résultats dans le fichier Excel...\", Fore.CYAN)\n",
    "\n",
    "    wide_rows = []\n",
    "    columns_order = (list(grooming_keys) + list(optimization_keys) +\n",
    "                     [\"time_grooming\", \"time_rigid\", \"time_optimization\", \"time_total\",\n",
    "                      \"compactness_95\", \"final_specificity_error\", \"final_generalization_error\"]\n",
    "                    )\n",
    "\n",
    "    for i, res in enumerate(all_results, start=1):\n",
    "        # on merge grooming_params + res[\"params\"]\n",
    "        row_dict = {}\n",
    "\n",
    "        # grooming\n",
    "        groom_p = res.get(\"grooming_params\", {})\n",
    "        for gk in grooming_keys:\n",
    "            row_dict[gk] = groom_p.get(gk, None)\n",
    "\n",
    "        # optimization\n",
    "        optim_p = res[\"params\"]\n",
    "        for ok in optimization_keys:\n",
    "            row_dict[ok] = optim_p.get(ok, None)\n",
    "\n",
    "        # times\n",
    "        t_groom = res.get(\"time_grooming\", 0)\n",
    "        t_rigid = res.get(\"time_rigid\", 0)\n",
    "\n",
    "        row_dict[\"time_grooming\"] = mm_ss_format(t_groom)\n",
    "        row_dict[\"time_rigid\"]    = mm_ss_format(t_rigid)\n",
    "\n",
    "        st = res.get(\"step_times\", {})\n",
    "        t_opt = st.get(\"optimization\", 0)\n",
    "        t_tot = res.get(\"total_execution_time\", 0)\n",
    "        row_dict[\"time_optimization\"] = mm_ss_format(t_opt)\n",
    "        row_dict[\"time_total\"]        = mm_ss_format(t_tot)\n",
    "\n",
    "        # metrics\n",
    "        mets = res[\"metrics\"]\n",
    "        row_dict[\"compactness_95\"] = mets[\"compactness_required\"]\n",
    "        spec = mets[\"specificity\"]\n",
    "        row_dict[\"final_specificity_error\"] = ( spec[-1] if len(spec)>0 else None )\n",
    "        gen = mets[\"generalization\"]\n",
    "        row_dict[\"final_generalization_error\"] = ( gen[-1] if len(gen)>0 else None )\n",
    "\n",
    "        # on l'ajoute\n",
    "        wide_rows.append(row_dict)\n",
    "\n",
    "    df_wide = pd.DataFrame(wide_rows, columns=columns_order)\n",
    "    # on va rename l'index => \"Run_1\", \"Run_2\", ...\n",
    "    run_names = [f\"Run_{i}\" for i in range(1, len(all_results)+1)]\n",
    "    df_wide.index = run_names\n",
    "\n",
    "    # on transpose\n",
    "    df_tall = df_wide.transpose()\n",
    "\n",
    "    # on fait un reset => la 1re col \"Field\"\n",
    "    df_tall.insert(0, \"Field\", df_tall.index)\n",
    "    df_tall.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 3) On crée le workbook\n",
    "    writer = pd.ExcelWriter(excel_filename, engine=\"openpyxl\")\n",
    "\n",
    "    # On écrit df_tall dans \"RESULTS\"\n",
    "    df_tall.to_excel(writer, sheet_name=\"RESULTS\", index=False)\n",
    "\n",
    "    ws_results = writer.book[\"RESULTS\"]\n",
    "\n",
    "    # -- Mieux présenter la page : \n",
    "    ws_results.column_dimensions[\"A\"].width = 25  # la col \"Field\"\n",
    "    # on met un width un peu plus large pour chaque col \"Run_i\"\n",
    "    for col_i in range(2, len(run_names)+2):\n",
    "        colL = ws_results.cell(row=1, column=col_i).column_letter\n",
    "        ws_results.column_dimensions[colL].width = 14\n",
    "\n",
    "    # 4) Placer TOUS LES PARAMS en colonnes D/E, en dessous ?\n",
    "    #    On va récolter tout l'univers (Grooming + Optim) => faire un listing trié\n",
    "    all_groom_keys = [\"ANTIALIAS_ITERATIONS\", \"ISO_SPACING\", \"PAD_SIZE\",\n",
    "                      \"PAD_VALUE\", \"ISO_VALUE\", \"ICP_ITERATIONS\"]\n",
    "    all_optim_keys = sorted(OPT_PARAMS.keys())\n",
    "    # On va lister grooming + optim keys, \n",
    "    row_start_params = df_tall.shape[0] + 3\n",
    "    row_cur = row_start_params\n",
    "    ws_results.cell(row=row_cur, column=4, value=\"---- GROOMING PARAMS ----\")\n",
    "    row_cur += 1\n",
    "    for gk in all_groom_keys:\n",
    "        val = None\n",
    "        if gk == \"ANTIALIAS_ITERATIONS\":\n",
    "            val = ANTIALIAS_ITERATIONS\n",
    "        elif gk == \"ISO_SPACING\":\n",
    "            val = str(ISO_SPACING)\n",
    "        elif gk == \"PAD_SIZE\":\n",
    "            val = PAD_SIZE\n",
    "        elif gk == \"PAD_VALUE\":\n",
    "            val = PAD_VALUE\n",
    "        elif gk == \"ISO_VALUE\":\n",
    "            val = ISO_VALUE\n",
    "        elif gk == \"ICP_ITERATIONS\":\n",
    "            val = ICP_ITERATIONS\n",
    "        else:\n",
    "            val = None\n",
    "\n",
    "        ws_results.cell(row=row_cur, column=4, value=gk)\n",
    "        ws_results.cell(row=row_cur, column=5, value=str(val))\n",
    "        row_cur += 1\n",
    "\n",
    "    row_cur += 1\n",
    "    ws_results.cell(row=row_cur, column=4, value=\"---- OPTIM PARAMS ----\")\n",
    "    row_cur += 1\n",
    "    for ok in all_optim_keys:\n",
    "        val = OPT_PARAMS.get(ok, None)\n",
    "        ws_results.cell(row=row_cur, column=4, value=ok)\n",
    "        ws_results.cell(row=row_cur, column=5, value=str(val))\n",
    "        row_cur += 1\n",
    "\n",
    "    # 5) Feuilles \"Run_i\"\n",
    "    for i, res in enumerate(all_results, start=1):\n",
    "        sheet_name = f\"Run_{i}\"\n",
    "        dummy_df = pd.DataFrame()\n",
    "        dummy_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        ws_run = writer.book[sheet_name]\n",
    "\n",
    "        # Graphs\n",
    "        run_dir = os.path.join(BASE_OUTPUT_DIR, f\"Run_{i}\")\n",
    "        plot_dir = os.path.join(run_dir, \"plots\")\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        mets = res[\"metrics\"]\n",
    "        cvar = mets[\"cumulative_variance\"]\n",
    "        specificity_data = mets[\"specificity\"]\n",
    "        general_data = mets[\"generalization\"]\n",
    "\n",
    "        compactness_img = _plot_metric_curve(cvar, \"Compactness\", \"Variance\", i, plot_dir)\n",
    "        specificity_img = _plot_metric_curve(specificity_data, \"Specificity Error\", \"Error\", i, plot_dir)\n",
    "        general_img     = _plot_metric_curve(general_data, \"Generalization Error\", \"Error\", i, plot_dir)\n",
    "\n",
    "        row_img1 = 1\n",
    "        row_img2 = 12\n",
    "        row_img3 = 23\n",
    "\n",
    "        imgA = ExcelImage(compactness_img); imgA.width, imgA.height = 310, 180\n",
    "        imgB = ExcelImage(specificity_img); imgB.width, imgB.height = 310, 180\n",
    "        imgC = ExcelImage(general_img);     imgC.width, imgC.height = 310, 180\n",
    "\n",
    "        ws_run.add_image(imgA, f\"A{row_img1}\")\n",
    "        ws_run.add_image(imgB, f\"A{row_img2}\")\n",
    "        ws_run.add_image(imgC, f\"A{row_img3}\")\n",
    "\n",
    "        ws_run.cell(row=row_img1+9, column=1, value=f\"Composantes pour 95%: {mets['compactness_required']}\")\n",
    "        if len(specificity_data) > 0:\n",
    "            ws_run.cell(row=row_img2+9, column=1, \n",
    "                        value=f\"Specificity final error: {specificity_data[-1]:.4f}\")\n",
    "        if len(general_data) > 0:\n",
    "            ws_run.cell(row=row_img3+9, column=1, \n",
    "                        value=f\"Generalization final error: {general_data[-1]:.4f}\")\n",
    "\n",
    "        pc = res[\"pca_projection\"]\n",
    "        shape_names = res.get(\"pca_shape_names\", [])\n",
    "        # On met PC1/PC2\n",
    "        start_row_pc = 34\n",
    "        pc1_pc2_df = pd.DataFrame(pc, columns=[\"PC1\",\"PC2\"])\n",
    "        pc1_pc2_df.to_excel(writer, sheet_name=sheet_name, startrow=start_row_pc, index=False)\n",
    "\n",
    "        scatter_img, df_map_idx = _plot_pca_scatter(pc, shape_names, i, plot_dir)\n",
    "        ws_run.add_image(ExcelImage(scatter_img), \"L1\")\n",
    "\n",
    "        df_map_idx.to_excel(writer, sheet_name=sheet_name, startrow=start_row_pc+5, startcol=10, index=False)\n",
    "\n",
    "    writer.close()\n",
    "    color_print(f\"Fichier Excel sauvegardé : {excel_filename}\", Fore.CYAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Script Principal (Double Boucle : Grooming & Optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= GROOMING Variation 1/2 =================\n",
      "{'ICP_ITERATIONS': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1. Acquire Data ---\n",
      "  Nombre de shapes : 4\n",
      "\n",
      "--- Step 2. Groom - Data Pre-processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grooming shapes: 100%|██████████| 4/4 [00:35<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Grooming time: 35.99s\n",
      "\n",
      "--- Step 3. Groom - Rigid Transformations ---\n",
      "  Image de référence trouvée : TEST2_FULGUR_008_181477_label_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcul des transformations:  75%|███████▌  | 3/4 [00:32<00:10, 10.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     color_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPipeline terminée en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms (global).\u001b[39m\u001b[38;5;124m\"\u001b[39m, Fore\u001b[38;5;241m.\u001b[39mGREEN, Style\u001b[38;5;241m.\u001b[39mBRIGHT)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 40\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m color_print(\u001b[38;5;28mstr\u001b[39m(groom_params), Fore\u001b[38;5;241m.\u001b[39mBLUE)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Rerun grooming\u001b[39;00m\n\u001b[0;32m     39\u001b[0m shape_seg_list, shape_filenames, dataset_ids, shape_names, \\\n\u001b[1;32m---> 40\u001b[0m rigid_transforms, groomed_files, t_groom, t_rigid \u001b[38;5;241m=\u001b[39m \u001b[43mrun_preprocessing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDATASET_PATHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSHAPE_EXT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroom_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrooming_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGroom_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mig\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# On va stocker t_groom, t_rigid => on le recopie ensuite sur chaque run\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m io, optim_params \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optim_combos, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[67], line 111\u001b[0m, in \u001b[0;36mrun_preprocessing\u001b[1;34m(dataset_paths, shape_ext, grooming_params, base_dir)\u001b[0m\n\u001b[0;32m    108\u001b[0m shape_seg_list, shape_names, t_groom \u001b[38;5;241m=\u001b[39m groom_shapes(shape_filenames, dataset_ids, groom_dir)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Rigid\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m rigid_transforms, groomed_files, t_rigid \u001b[38;5;241m=\u001b[39m \u001b[43mrigid_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape_seg_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroom_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (shape_seg_list, shape_filenames, dataset_ids, shape_names, \n\u001b[0;32m    114\u001b[0m         rigid_transforms, groomed_files, t_groom, t_rigid)\n",
      "Cell \u001b[1;32mIn[67], line 59\u001b[0m, in \u001b[0;36mrigid_transformations\u001b[1;34m(shape_seg_list, shape_names, groom_dir)\u001b[0m\n\u001b[0;32m     56\u001b[0m     transform_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(transform_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_to_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_transform.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m     np\u001b[38;5;241m.\u001b[39msavetxt(transform_filename, rigid_transform)\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mshape_seg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mANTIALIAS_ITERATIONS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeDT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussianBlur\u001b[49m(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     61\u001b[0m output_subdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_transforms\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     62\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(groom_dir, output_subdir)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global_start = time.time()\n",
    "    all_results = []\n",
    "    run_counter = 0\n",
    "\n",
    "    grooming_dir = os.path.join(BASE_OUTPUT_DIR, \"GROOMING\")\n",
    "    os.makedirs(grooming_dir, exist_ok=True)\n",
    "\n",
    "    # Constructions de combinaisons de grooming\n",
    "    grooming_keys = list(GRID_GROOMING.keys())\n",
    "    grooming_values = [GRID_GROOMING[k] for k in grooming_keys]\n",
    "    if not grooming_keys:\n",
    "        grooming_combos = [{}]\n",
    "    else:\n",
    "        grooming_combos = []\n",
    "        for combo_g in itertools.product(*grooming_values):\n",
    "            d = {}\n",
    "            for k, val in zip(grooming_keys, combo_g):\n",
    "                d[k] = val\n",
    "            grooming_combos.append(d)\n",
    "\n",
    "    # Constructions de combinaisons d'optim\n",
    "    optim_keys = list(GRID_OPTIMIZATION.keys())\n",
    "    optim_values = [GRID_OPTIMIZATION[k] for k in optim_keys]\n",
    "    optim_combos = []\n",
    "    for combo_o in itertools.product(*optim_values):\n",
    "        dd = {}\n",
    "        for k, val in zip(optim_keys, combo_o):\n",
    "            dd[k] = val\n",
    "        optim_combos.append(dd)\n",
    "\n",
    "    # On boucle\n",
    "    for ig, groom_params in enumerate(grooming_combos, start=1):\n",
    "        color_print(f\"\\n================= GROOMING Variation {ig}/{len(grooming_combos)} =================\",\n",
    "                    Fore.BLUE, Style.BRIGHT)\n",
    "        color_print(str(groom_params), Fore.BLUE)\n",
    "\n",
    "        # Rerun grooming\n",
    "        shape_seg_list, shape_filenames, dataset_ids, shape_names, \\\n",
    "        rigid_transforms, groomed_files, t_groom, t_rigid = run_preprocessing(\n",
    "            DATASET_PATHS, SHAPE_EXT, groom_params,\n",
    "            base_dir=os.path.join(grooming_dir, f\"Groom_{ig}\")\n",
    "        )\n",
    "\n",
    "        # On va stocker t_groom, t_rigid => on le recopie ensuite sur chaque run\n",
    "        for io, optim_params in enumerate(optim_combos, start=1):\n",
    "            run_counter += 1\n",
    "            color_print(f\"\\n  >>> RUN {run_counter} / G={ig}, O={io} <<<\", Fore.MAGENTA, Style.BRIGHT)\n",
    "            color_print(\"   Optim params:\" + str(optim_params), Fore.MAGENTA)\n",
    "\n",
    "            # Exécuter\n",
    "            out = run_optimization_and_analysis(\n",
    "                run_params=optim_params,\n",
    "                run_index=run_counter,\n",
    "                shape_seg_list=shape_seg_list,\n",
    "                shape_filenames=shape_filenames,\n",
    "                rigid_transforms=rigid_transforms,\n",
    "                groomed_files=groomed_files,\n",
    "                base_output_dir=BASE_OUTPUT_DIR\n",
    "            )\n",
    "            # On ajoute l'info grooming\n",
    "            out[\"grooming_params\"] = groom_params\n",
    "            out[\"time_grooming\"]   = t_groom\n",
    "            out[\"time_rigid\"]      = t_rigid\n",
    "            all_results.append(out)\n",
    "\n",
    "    # Export excel\n",
    "    excel_filename = os.path.join(BASE_OUTPUT_DIR, \"grid_search_results.xlsx\")\n",
    "    save_results_to_excel(all_results, excel_filename, grooming_keys, optim_keys)\n",
    "\n",
    "    total_time = time.time() - global_start\n",
    "    color_print(f\"\\nPipeline terminée en {total_time:.2f}s (global).\", Fore.GREEN, Style.BRIGHT)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
